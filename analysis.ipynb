{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <p style=\"text-align: center\"> Invisible Influencers </p>\n",
    "## <p style=\"text-align: center\"> Investigating YouTube Bot's Phenomenon </p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import glob\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Ensure plotly.js is set to default\n",
    "pio.kaleido.scope.plotlyjs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Default Plotting Params\n",
    "\n",
    "font_family = \"Arial\"  # Replace with your desired font\n",
    "plt.rcParams[\"font.family\"] = font_family\n",
    "\n",
    "# Optional: Set font size and weight\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "\n",
    "# Apply Seaborn theme (inherits Matplotlib fonts)\n",
    "sns.set_theme(style=\"whitegrid\", font=font_family)\n",
    "\n",
    "# Apply Plotly theme\n",
    "plotly_layout = {\n",
    "    \"font\": {\n",
    "        \"family\": font_family,\n",
    "        \"size\": 12,  # Match with plt.rcParams[\"font.size\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center\"> <span style=\"text-decoration: underline\"> **Type-1 Bot Analysis** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Data Preprocessing and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "\n",
    "if combine:\n",
    "\n",
    "    datasets = ['normal', 'suspicious']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "        # List all Parquet files\n",
    "        parquet_files = glob.glob(f'./data_type1/{dataset}_users_*.parquet')\n",
    "\n",
    "        # Read and concatenate all Parquet files\n",
    "        combined = pl.concat([pl.read_parquet(file) for file in parquet_files])\n",
    "        combined.write_parquet(f'data_type1/combi_{dataset}_dataset1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "type_1=pl.read_parquet('data/data_type1/combi_dataset1.parquet')\n",
    "df_sus=pl.read_parquet('data/data_type1/combi_suspicious_dataset1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'year' to the dataframe\n",
    "\n",
    "type_1 = type_1.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])\n",
    "\n",
    "df_sus = df_sus.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])\n",
    "\n",
    "df_sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Target Specific Video Categories ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap for consistency\n",
    "\n",
    "colormap_categories = {\n",
    "    \"Entertainment\": \"red\",\n",
    "    \"Other\": \"orange\",\n",
    "    \"Gaming\": \"cyan\",\n",
    "    \"People & Blogs\": \"yellow\",\n",
    "    \"News & Politics\": \"lime\",\n",
    "    \"Howto & Style\": \"lightblue\",\n",
    "    \"Music\": \"blue\",\n",
    "    \"Education\": \"lightgreen\",\n",
    "    \"Science & Technology\": \"brown\",\n",
    "    \"Film & Animation\": \"pink\",\n",
    "    \"Comedy\": \"green\",\n",
    "    \"Sports\": \"purple\",\n",
    "    \"Pets & Animals\": \"teal\",\n",
    "    \"Travel & Events\": \"lavender\",\n",
    "    \"Autos & Vehicles\": \"salmon\",\n",
    "    \"Nonprofits & Activism\": \"gold\",\n",
    "    \"Shows\": \"gold\",\n",
    "    \"Trailers\": \"lightcoral\"\n",
    "}\n",
    "\n",
    "order_categories = {\n",
    "    \"Entertainment\": 1,\n",
    "    \"Gaming\": 2,\n",
    "    \"People & Blogs\": 3,\n",
    "    \"News & Politics\": 4,\n",
    "    \"Howto & Style\": 5,\n",
    "    \"Music\": 6,\n",
    "    \"Education\": 7,\n",
    "    \"Science & Technology\": 8,\n",
    "    \"Film & Animation\": 9,\n",
    "    \"Comedy\": 10,\n",
    "    \"Sports\": 11,\n",
    "    \"Pets & Animals\": 12,\n",
    "    \"Travel & Events\": 13,\n",
    "    \"Autos & Vehicles\": 14,\n",
    "    \"Nonprofits & Activism\": 15,\n",
    "    \"Shows\": 16,\n",
    "    \"Trailers\": 17,\n",
    "    \"Other\": 18\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of comments per category of videos\n",
    "df_comm_per_cat = df_sus.group_by([\"year\",\"categories\"]).agg(pl.col(\"comments\").sum()).filter(pl.col(\"categories\")!=\"\").sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add proportion of comments per category per year\n",
    "comments_per_year = df_comm_per_cat.group_by(\"year\").agg(pl.col(\"comments\").sum().alias(\"total_comments\")).sort(by=\"year\")\n",
    "df_comm_per_cat = df_comm_per_cat.join(comments_per_year, on=\"year\")\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns([\n",
    "    (pl.col(\"comments\") / pl.col(\"total_comments\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "# Keep only categories with more than 2% of the comments, put others in a category 'Other'\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns(pl.when(pl.col(\"proportion\")<5.0).then(pl.lit(\"Other\")).otherwise(pl.col(\"categories\")).alias(\"Categories\")).drop(\"categories\")\n",
    "df_comm_per_cat = df_comm_per_cat.group_by([\"year\",\"Categories\"]).agg(pl.col(\"comments\").sum(), pl.col(\"proportion\").sum()).sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add order column for plotting\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns([\n",
    "    pl.col(\"Categories\").map_elements(lambda x: order_categories[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_comm_per_cat = df_comm_per_cat.sort(by=[\"year\",\"order\"])\n",
    "\n",
    "df_comm_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_comm_per_cat.to_pandas(), x=\"year\", y=\"proportion\", color=\"Categories\", \n",
    "              labels={\"proportion\":\"Proportion of Comments (%)\", \"year\":\"Year\", \"Categories\":\"Category\"},\n",
    "              color_discrete_map=colormap_categories)\n",
    "fig.show()\n",
    "fig.write_image(\"./image_aurel/prop_comments_per_category_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_comments_per_category_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of comments per category of videos\n",
    "df_comm_per_cat_normal = type_1.group_by([\"year\",\"categories\"]).agg(pl.col(\"comments\").sum()).filter(pl.col(\"categories\")!=\"\").sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add proportion of comments per category per year\n",
    "comments_per_year_normal = df_comm_per_cat_normal.group_by(\"year\").agg(pl.col(\"comments\").sum().alias(\"total_comments\")).sort(by=\"year\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.join(comments_per_year_normal, on=\"year\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns([\n",
    "    (pl.col(\"comments\") / pl.col(\"total_comments\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "# Keep only categories with more than 4% of the comments, put others in a category 'Other'\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns(pl.when(pl.col(\"proportion\")<5.0).then(pl.lit(\"Other\")).otherwise(pl.col(\"categories\")).alias(\"Categories\")).drop(\"categories\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.group_by([\"year\",\"Categories\"]).agg(pl.col(\"comments\").sum(), pl.col(\"proportion\").sum()).sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add order column for plotting\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns([\n",
    "    pl.col(\"Categories\").map_elements(lambda x: order_categories[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_comm_per_cat_normal.to_pandas(), x=\"year\", y=\"proportion\", color=\"Categories\", \n",
    "            labels={\"proportion\":\"Proportion of Comments (%)\", \"year\":\"Year\", \"Categories\":\"Category\"},\n",
    "            color_discrete_map=colormap_categories)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_comments_per_category_normal.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_comments_per_category_normal.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Target One or Many Channels ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map for consistency\n",
    "\n",
    "colormap_cat_channel = {\n",
    "    \"1\": \"red\",\n",
    "    \"2\": \"blue\",\n",
    "    \"3\": \"green\",\n",
    "    \"4\": \"orange\",\n",
    "    \"5+\": \"cyan\",\n",
    "}\n",
    "\n",
    "order_cat_channel = {\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5+\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_channels_per_bot = df_sus.group_by([\"year\",\"author\"]).agg(pl.col(\"channel_id\").n_unique().alias(\"nb_channels\")).sort(by=[\"year\",\"nb_channels\"], descending=True)\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns(pl.when(pl.col(\"nb_channels\")>=5).then(pl.lit(\"5+\")).otherwise(pl.col(\"nb_channels\")).alias(\"nb_channels\")).group_by([\"year\",\"nb_channels\"]).agg(pl.col(\"nb_channels\").count().alias(\"nb_users\")).sort(by=[\"year\",\"nb_users\"], descending=True)\n",
    "\n",
    "# Proportion per year \n",
    "\n",
    "nb_bots_per_year = df_nb_channels_per_bot.group_by(\"year\").agg(pl.col(\"nb_users\").sum().alias(\"total_users\")).sort(by=\"year\")\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.join(nb_bots_per_year, on=\"year\")\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns([\n",
    "    (pl.col(\"nb_users\") / pl.col(\"total_users\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns([\n",
    "    pl.col(\"nb_channels\").map_elements(lambda x: order_cat_channel[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_nb_channels_per_bot.to_pandas(), x=\"year\", y=\"proportion\", color=\"nb_channels\", \n",
    "              labels={\"proportion\":\"Proportion of Bots (%)\", \"year\":\"Year\", \"nb_channels\":\"Channels Targeted\"},\n",
    "              color_discrete_map=colormap_cat_channel)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_bots_per_channels_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_bots_per_channels_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_channels_per_user = type_1.group_by([\"year\",\"author\"]).agg(pl.col(\"channel_id\").n_unique().alias(\"nb_channels\")).sort(by=[\"year\",\"nb_channels\"], descending=True)\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns(pl.when(pl.col(\"nb_channels\")>=5).then(pl.lit(\"5+\")).otherwise(pl.col(\"nb_channels\")).alias(\"nb_channels\")).group_by([\"year\",\"nb_channels\"]).agg(pl.col(\"nb_channels\").count().alias(\"nb_users\")).sort(by=[\"year\",\"nb_users\"], descending=True)\n",
    "\n",
    "# Proportion per year \n",
    "\n",
    "nb_user_per_year = df_nb_channels_per_user.group_by(\"year\").agg(pl.col(\"nb_users\").sum().alias(\"total_users\")).sort(by=\"year\")\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.join(nb_user_per_year, on=\"year\")\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns([\n",
    "    (pl.col(\"nb_users\") / pl.col(\"total_users\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns([\n",
    "    pl.col(\"nb_channels\").map_elements(lambda x: order_cat_channel[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_nb_channels_per_user.to_pandas(), x=\"year\", y=\"proportion\", color=\"nb_channels\", \n",
    "              labels={\"proportion\":\"Proportion of Normal users (%)\", \"year\":\"Year\", \"nb_channels\":\"Channels Targeted\"},\n",
    "              color_discrete_map=colormap_cat_channel)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_normal_per_channels.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_normal_per_channels.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Different are Normal Users and Bots in Commenting Behaviors ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type1_comm_per_user = type_1.select([\"author\", \"comments\"]).group_by(\"author\").agg(pl.col(\"comments\").sum()).sort(by=\"comments\", descending=True)\n",
    "\n",
    "df_sus_comm_per_bot= df_sus.select([\"author\", \"comments\"]).group_by(\"author\").agg(pl.col(\"comments\").sum()).sort(by=\"comments\", descending=True)\n",
    "\n",
    "df_comm_per_bot = pl.concat([data_type1_comm_per_user.with_columns([\n",
    "    pl.lit(\"Normal\").alias(\"Type\")\n",
    "]), df_sus_comm_per_bot.with_columns([\n",
    "    pl.lit(\"Suspicious\").alias(\"Type\")\n",
    "])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot of comments per user for normal and suspicious users\n",
    "\n",
    "fig = px.box(df_comm_per_bot.to_pandas(), x=\"Type\", y=\"comments\", color=\"Type\",\n",
    "         labels={\"comments\":\"Number of Comments\", \"Type\":\"User Type\"},\n",
    "         log_y=True)\n",
    "fig.update_yaxes(range=[None, 10**5])\n",
    "\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"./image_aurel/boxplot_comments_per_user.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Do Metrics Vary Over Time For Normal Users and Bots ?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Metrics Data\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size_sus = 1_000_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results_sus = []\n",
    "chunk_results_sus = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start_sus in range(0, len(df_sus), chunk_size_sus):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus[start_sus : start_sus + chunk_size_sus]\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill_sus = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col_sus in columns_to_fill_sus:\n",
    "        if col_sus in chunk_sus.columns:\n",
    "            chunk_sus = chunk_sus.with_columns(pl.col(col_sus).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results_sus.append(chunk_metrics_sus)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics_sus = pl.concat(chunk_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start_sus in range(0, len(df_sus), chunk_size_sus):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus[start_sus : start_sus + chunk_size_sus]\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results_sus.append(unique_users_sus)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined_sus = pl.concat(unique_users_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result_sus = final_metrics_sus.join(unique_users_combined_sus, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result_sus = final_result_sus.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")\n",
    "# Sort the DataFrame by year\n",
    "final_result_sus = final_result_sus.sort(\"year\")\n",
    "# Display the final result\n",
    "print(final_result_sus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics\n",
    "\n",
    "final_result_sus = final_result_sus.with_columns([\n",
    "    (pl.col(\"total_likes\") / final_result_sus[\"total_likes\"].max()).alias(\"Total Likes\"),\n",
    "    (pl.col(\"total_comments\") / final_result_sus[\"total_comments\"].max()).alias(\"Total Comments\"),\n",
    "    (pl.col(\"comments_per_user\") / final_result_sus[\"comments_per_user\"].max()).alias(\"Comments per User\"),\n",
    "    (pl.col(\"total_replies\") / final_result_sus[\"total_replies\"].max()).alias(\"Total Replies\"),\n",
    "])\n",
    "\n",
    "# Remove year 2005\n",
    "final_result_sus = final_result_sus.filter(pl.col(\"year\")>2005)\n",
    "\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"cyan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(final_result_sus.to_pandas(), x=\"year\", y=[\"Total Likes\", \"Total Comments\", \"Comments per User\", \"Total Replies\"],\n",
    "                labels={\"value\":\"Normalized Value\", \"year\":\"Year\", \"variable\":\"Metric\"},\n",
    "                color_discrete_sequence=colors, markers=True)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = [2006+x for x in range(0,13, 2)],\n",
    "        ticktext = [str(2006+x) for x in range(0,13, 2)]\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"./image_aurel/normalized_metrics_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/normalized_metrics_sus.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Metrics Data\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1_000_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results = []\n",
    "chunk_results = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start in range(0, len(type_1), chunk_size):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = type_1[start : start + chunk_size]\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col in columns_to_fill:\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk.with_columns(pl.col(col).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results.append(chunk_metrics)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics = pl.concat(chunk_results).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start in range(0, len(type_1), chunk_size):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = type_1[start : start + chunk_size]\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results.append(unique_users)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined = pl.concat(unique_users_results).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result = final_metrics.join(unique_users_combined, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result = final_result.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")\n",
    "\n",
    "# Display the final result\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics\n",
    "\n",
    "final_result = final_result.with_columns([\n",
    "    (pl.col(\"total_likes\") / final_result[\"total_likes\"].max()).alias(\"Total Likes\"),\n",
    "    (pl.col(\"total_comments\") / final_result[\"total_comments\"].max()).alias(\"Total Comments\"),\n",
    "    (pl.col(\"comments_per_user\") / final_result[\"comments_per_user\"].max()).alias(\"Comments per User\"),\n",
    "    (pl.col(\"total_replies\") / final_result[\"total_replies\"].max()).alias(\"Total Replies\"),\n",
    "])\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"cyan\"]\n",
    "markers = {\n",
    "    \"Total Likes\": \"circle\",\n",
    "    \"Total Comments\": \"square\",\n",
    "    \"Comments per User\": \"diamond\",\n",
    "    \"Total Replies\": \"triangle-up\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(final_result.to_pandas(), x=\"year\", y=[\"Total Likes\", \"Total Comments\", \"Comments per User\", \"Total Replies\"],\n",
    "                labels={\"value\":\"Normalized Value\", \"year\":\"Year\", \"variable\":\"Metric\"},\n",
    "                color_discrete_sequence=colors, markers=True)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = [2006+x for x in range(0,13, 2)],\n",
    "        ticktext = [str(2006+x) for x in range(0,13, 2)]\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"./image_aurel/normalized_metrics.svg\")\n",
    "fig.write_html(\"./image_aurel/normalized_metrics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How different are bots and normal users?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter suspicious users to retain only the necessary columns\n",
    "data_com_vid_year_sus= df_sus.select([\"year\", \"author\", \"comments\", \"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and author to compute metrics\n",
    "\n",
    "grouped_data_sus = (\n",
    "    data_com_vid_year_sus.group_by([\"year\", \"author\"])  # Group by year and author\n",
    "    .agg([\n",
    "        pl.col(\"comments\").sum().alias(\"total_comments\"),  # Total comments by user per year\n",
    "        pl.col(\"video_id\").n_unique().alias(\"distinct_videos_commented\"),  # Unique videos commented on\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"total_comments\") / pl.col(\"distinct_videos_commented\")).alias(\"avg_comments_per_user\")  # Avg comments per user\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample suspicious users because the plot would be impossible to make :\n",
    "\n",
    "# Group data by year\n",
    "grouped_sus = grouped_data_sus.group_by('year')\n",
    "\n",
    "# Calculate the total subsample size (1/100th of the total rows)\n",
    "total_subsample_size = len(grouped_data_sus) // 1\n",
    "\n",
    "# Subsampling uniformly across years\n",
    "subsampled_list_sus = []\n",
    "for year, group in grouped_sus:\n",
    "    # Ensure `group` is a DataFrame\n",
    "    group = pd.DataFrame(group)\n",
    "    \n",
    "    # Calculate the number of samples for this year (proportional to the group's size)\n",
    "    year_sample_size = max(1, len(group) * total_subsample_size // len(grouped_data_sus))\n",
    "    \n",
    "    # Randomly sample the data for this year\n",
    "    subsampled_list_sus.append(group.sample(n=year_sample_size, random_state=42))\n",
    "\n",
    "# Combine the sampled data from all years\n",
    "subsampled_df_sus = pd.concat(subsampled_list_sus)\n",
    "\n",
    "# Define the correct column names\n",
    "column_names = ['year', 'author', 'total_comments', 'distinct_videos_commented', 'avg_comments_per_user']\n",
    "\n",
    "# Assign the column names back to the DataFrame\n",
    "subsampled_df_sus.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the year column is integer for proper sorting FOR SUSPICIOUS USERS\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "subsampled_df_sus = subsampled_df_sus.sort_values(by='year', ascending=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "subsampled_df_sus = subsampled_df_sus.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Line Plot for Suspicious Users\n",
    "\n",
    "# Ensure Seaborn has a clean theme\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Ensure 'year' is treated as categorical for the plot\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(int)  # Convert years to integers\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(str)  # Convert to string for categorical use\n",
    "\n",
    "# Generate a color palette for years\n",
    "pal = sns.color_palette(palette='viridis', n_colors=subsampled_df_sus['year'].nunique())\n",
    "\n",
    "# Create the FacetGrid for ridgeline\n",
    "g = sns.FacetGrid(\n",
    "    subsampled_df_sus,\n",
    "    row='year',\n",
    "    hue='year',\n",
    "    aspect=15,  # Stretch plots horizontally\n",
    "    height=0.5,  # Adjust height of each row\n",
    "    palette=pal,\n",
    ")\n",
    "\n",
    "# Add density plots (kde)\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=0.2,\n",
    "    clip=(0, 50),  # Clip x-axis range to 0-5\n",
    "    #clip_on=False,\n",
    "    fill=True,\n",
    "    alpha=1,\n",
    "    linewidth=1.5,\n",
    ")\n",
    "# g.set_titles(\"\")\n",
    "# g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Add a white contour line around each density plot\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=0.2,\n",
    "    clip=(0, 50),  # Clip x-axis range to 0-5\n",
    "    #clip_on=False,  \n",
    "    color=\"w\",\n",
    "    lw=2,\n",
    ")\n",
    "\n",
    "# Add a horizontal line at y=0 for each plot\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Add year labels to each plot\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.text(\n",
    "        -0.5, 0.02,  # Adjust the position of the year label\n",
    "        subsampled_df_sus['year'].unique()[i],\n",
    "        fontweight='bold',\n",
    "        fontsize=12,\n",
    "        color=ax.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "# Adjust subplot overlap\n",
    "g.fig.subplots_adjust(hspace=-0.5)\n",
    "\n",
    "# Remove the density label from each y-axis\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylabel(\"\")  # Remove the y-axis labels for density\n",
    "    ax.set_xlim(0, 50)  # Set x-axis range to 0 - 50    \n",
    "\n",
    "\n",
    "# Remove unnecessary axes details\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel(\"Average Comments per User\", fontweight='bold', fontsize=12)\n",
    "g.fig.suptitle('Distribution of Avg Comments per User Across Years',\n",
    "               ha='right', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "g.savefig(\"./image_aurel/ridge_line_sus.svg\")\n",
    "g.savefig(\"./image_aurel/ridge_line_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter normal users to retain only the necessary columns\n",
    "data_com_vid_year= type_1.select([\"year\", \"author\", \"comments\", \"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and author to compute metrics\n",
    "grouped_data = (\n",
    "    data_com_vid_year.group_by([\"year\", \"author\"])  # Group by year and author\n",
    "    .agg([\n",
    "        pl.col(\"comments\").sum().alias(\"total_comments\"),  # Total comments by user per year\n",
    "        pl.col(\"video_id\").n_unique().alias(\"distinct_videos_commented\"),  # Unique videos commented on\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"total_comments\") / pl.col(\"distinct_videos_commented\")).alias(\"avg_comments_per_user\")  # Avg comments per user\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample normal users because the plot would be impossible to make :\n",
    "\n",
    "# Group data by year\n",
    "grouped = grouped_data.group_by('year')\n",
    "\n",
    "# Calculate the total subsample size (1/100th of the total rows)\n",
    "total_subsample_size = len(grouped_data) // 100\n",
    "\n",
    "# Subsampling uniformly across years\n",
    "subsampled_list = []\n",
    "for year, group in grouped:\n",
    "    # Ensure `group` is a DataFrame\n",
    "    group = pd.DataFrame(group)\n",
    "    \n",
    "    # Calculate the number of samples for this year (proportional to the group's size)\n",
    "    year_sample_size = max(1, len(group) * total_subsample_size // len(grouped_data))\n",
    "    \n",
    "    # Randomly sample the data for this year\n",
    "    subsampled_list.append(group.sample(n=year_sample_size, random_state=42))\n",
    "\n",
    "# Combine the sampled data from all years\n",
    "subsampled_df = pd.concat(subsampled_list)\n",
    "\n",
    "# Define the correct column names\n",
    "column_names = ['year', 'author', 'total_comments', 'distinct_videos_commented', 'avg_comments_per_user']\n",
    "\n",
    "# Assign the column names back to the DataFrame\n",
    "subsampled_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Line Plot for Normal Users\n",
    "\n",
    "# Ensure the year column is integer for proper sorting\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "subsampled_df = subsampled_df.sort_values(by='year', ascending=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "subsampled_df = subsampled_df.reset_index(drop=True)\n",
    "\n",
    "# Ensure Seaborn has a clean theme\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Prepare data (already in `subsampled_df`)\n",
    "# Ensure 'year' is treated as categorical for the plot\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(int)  # Convert years to integers\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(str)  # Convert to string for categorical use\n",
    "\n",
    "# Generate a color palette for years\n",
    "pal = sns.color_palette(palette='viridis', n_colors=subsampled_df['year'].nunique())\n",
    "\n",
    "# Create the FacetGrid for ridgeline\n",
    "g = sns.FacetGrid(\n",
    "    subsampled_df,\n",
    "    row='year',\n",
    "    hue='year',\n",
    "    aspect=15,  # Stretch plots horizontally\n",
    "    height=0.5,  # Adjust height of each row\n",
    "    palette=pal,\n",
    ")\n",
    "\n",
    "# Add density plots (kde)\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=1,  # Bandwidth adjustment\n",
    "    clip_on=False,\n",
    "    fill=True,\n",
    "    alpha=1,\n",
    "    linewidth=1.5,\n",
    ")\n",
    "# g.set_titles(\"\")\n",
    "# g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Add a white contour line around each density plot\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=1,\n",
    "    clip_on=False,  \n",
    "    color=\"w\",\n",
    "    lw=2,\n",
    ")\n",
    "\n",
    "# Add a horizontal line at y=0 for each plot\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Add year labels to each plot\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.text(\n",
    "        -0.5, 0.02,  # Adjust the position of the year label\n",
    "        subsampled_df['year'].unique()[i],\n",
    "        fontweight='bold',\n",
    "        fontsize=12,\n",
    "        color=ax.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "# Adjust subplot overlap\n",
    "g.fig.subplots_adjust(hspace=-0.5)\n",
    "\n",
    "# Remove the density label from each y-axis\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylabel(\"\")  # Remove the y-axis labels for density\n",
    "\n",
    "# Remove unnecessary axes details\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel(\"Average Comments per User\", fontweight='bold', fontsize=12)\n",
    "g.fig.suptitle('Distribution of Avg Comments per User Across Years',\n",
    "               ha='right', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "g.savefig(\"./image_aurel/ridge_line_normal.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **3D Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine both datasets for SVM training\n",
    "combined_df = pd.concat([\n",
    "    subsampled_df.assign(label=0),  # Normal users\n",
    "    subsampled_df_sus.assign(label=1)  # Suspicious users\n",
    "])\n",
    "\n",
    "# Combine features and labels\n",
    "X = combined_df[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values\n",
    "y = combined_df[\"label\"].values\n",
    "\n",
    "# Train the SVM\n",
    "svm = SVC(kernel=\"linear\", C=1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Extract hyperplane parameters\n",
    "w = svm.coef_[0]  # Weights\n",
    "b = svm.intercept_[0]  # Intercept\n",
    "\n",
    "# Create mesh grid for the hyperplane\n",
    "x_range = np.linspace(combined_df[\"year\"].min(), combined_df[\"year\"].max(), 30)\n",
    "y_range = np.linspace(combined_df[\"avg_comments_per_user\"].min(), combined_df[\"avg_comments_per_user\"].max(), 30)\n",
    "x, y = np.meshgrid(x_range, y_range)\n",
    "z = (-w[0] * x - w[1] * y - b) / w[2]  # Solve for z\n",
    "\n",
    "# Density calculation for Dataset 1\n",
    "xyz1 = subsampled_df[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values.T\n",
    "kde1 = gaussian_kde(xyz1)(xyz1)\n",
    "subsampled_df[\"density\"] = kde1\n",
    "\n",
    "# Density calculation for Dataset 2\n",
    "xyz2 = subsampled_df_sus[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values.T\n",
    "kde2 = gaussian_kde(xyz2)(xyz2)\n",
    "subsampled_df_sus[\"density\"] = kde2\n",
    "\n",
    "# Create the 3D Scatter Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Dataset 1 (Normal Users) with density-based coloring\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=subsampled_df[\"year\"],\n",
    "    y=subsampled_df[\"avg_comments_per_user\"],\n",
    "    z=subsampled_df[\"distinct_videos_commented\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=subsampled_df[\"density\"],  # Color based on density\n",
    "        colorscale=\"Viridis\",  # Blue-Green color scale\n",
    "        opacity=0.6\n",
    "    ),\n",
    "    name=\"Normal Users\"\n",
    "))\n",
    "\n",
    "# Add Dataset 2 (Suspicious Users) with density-based coloring\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=subsampled_df_sus[\"year\"],\n",
    "    y=subsampled_df_sus[\"avg_comments_per_user\"],\n",
    "    z=subsampled_df_sus[\"distinct_videos_commented\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=subsampled_df_sus[\"density\"],  # Color based on density\n",
    "        colorscale=\"Jet\",  # Yellow-to-Red color scale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name=\"Suspicious Users\"\n",
    "))\n",
    "\n",
    "# Add the SVM hyperplane\n",
    "fig.add_trace(go.Surface(\n",
    "    x=x, y=y, z=z,\n",
    "    colorscale=[[0, 'lightgrey'], [1, 'lightgrey']],  # Single light-grey color\n",
    "    opacity=0.5,\n",
    "    showscale=False,  # Remove colorbar for hyperplane\n",
    "    name=\"SVM Hyperplane\"\n",
    "))\n",
    "\n",
    "# Add a box with the hyperplane metrics\n",
    "fig.add_annotation(\n",
    "    text=f\"<b>SVM Hyperplane Metrics</b><br>\"\n",
    "         f\"Equation: {w[0]:.2f}*x1 + {w[1]:.2f}*x2 + {w[2]:.2f}*x3 + {b:.2f} = 0\",\n",
    "    showarrow=False,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.05, y=0.95,  # Position: top-left corner\n",
    "    bordercolor=\"black\", borderwidth=1,\n",
    "    bgcolor=\"white\", font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"Year\", tickvals=list(range(2005, 2020)), autorange=\"reversed\"),\n",
    "        yaxis=dict(title=\"Average Comments/User\", range=[0, 50]),\n",
    "        zaxis=dict(title=\"Distinct Videos Commented\", range=[0, 1000])\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "fig.write_html(\"./image_aurel/3D_hyperplane_density.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center\"> <span style=\"text-decoration: underline\"> **Type-2 Bot Analysis** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Data Preprocessing and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "\n",
    "if combine:\n",
    "\n",
    "    datasets = ['normal', 'suspicious']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "        # List all Parquet files\n",
    "        parquet_files = glob.glob(f'./data/data_type2/{dataset}_users_*.parquet')\n",
    "\n",
    "        # Read and concatenate all Parquet files\n",
    "        combined = pl.concat([pl.read_parquet(file) for file in parquet_files])\n",
    "        combined.write_parquet(f'./data/data_type2/combi_{dataset}_dataset2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_2 = pl.read_parquet('./data/data_type2/combi_normal_dataset2.parquet')\n",
    "df_sus_2 = pl.read_parquet('./data/data_type2/combi_suspicious_dataset2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Comment On More Videos ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.read_parquet(df_normal_2)[0] # Just need 1 df for plot\n",
    "df_sus =  pd.read_parquet(df_sus_2)[0]\n",
    "df_sus = df_sus[df_sus[\"videos_commented\"] >= 10]\n",
    "\n",
    "# Adding a column to discriminate between the 2 df\n",
    "df_norm[\"category\"]='normal'\n",
    "df_sus[\"category\"]='suspicious'\n",
    "\n",
    "# Concatenating for the plot\n",
    "df = pd.concat([df_norm[::5],df_sus[df_sus[\"videos_commented\"] < 30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.violinplot(x=\"category\", y=\"videos_commented\", hue=\"category\", data=df, palette=\"Pastel1\", split=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for category in ['normal', 'suspicious']:\n",
    "    fig.add_trace(go.Violin(x=df['category'][df['category'] == category],\n",
    "                            y=df['videos_commented'][df['category'] == category],\n",
    "                            name=category,\n",
    "                            box_visible=True,\n",
    "                            meanline_visible=True))\n",
    "\n",
    "fig.update_traces(meanline_visible=True)\n",
    "fig.update_layout(violingap=0, violinmode='overlay', yaxis=dict(\n",
    "        title=dict(\n",
    "            text=\"Number of videos commented\"\n",
    "        )\n",
    "    ),)\n",
    "fig.write_html(\"images/dist_num_comments_norm_sus.html\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Does The Number of Comments and Videos Commented Vary Over Time ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'suspicious'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_files_type_2 = glob.glob(f'./data/data_type2/{dataset}_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Save Time Series Data\n",
    "\n",
    "result = pl.concat([pl.scan_parquet(dataset).group_by('upload_date').agg([\n",
    "                    pl.col(\"videos_commented\").sum(),\n",
    "                    pl.col(\"author\").unique().count()]).collect(streaming=True)\n",
    "                    for dataset in tqdm(parquet_files_type_2, desc='Processing chunks')\n",
    "                    ]).group_by('upload_date').agg([pl.col(\"videos_commented\").sum(),\n",
    "                    pl.col(\"author\").unique().count()]).rename({'author' : 'distinct_bots'})\n",
    "\n",
    "result = result.with_columns(comments_per_bot = result['videos_commented'] / result['distinct_bots'])\n",
    "result.write_parquet('./data/data_type2/time_series_type_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Time Series Data\n",
    "\n",
    "result = pl.read_parquet('./data/data_type2/time_series_type_2.parquet')\n",
    "result = result.sort('upload_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month\n",
    "result_monthly = result.to_pandas().groupby(pd.Grouper(key='upload_date', freq='MS')).agg(\n",
    "    videos_commented=('videos_commented', 'sum'),\n",
    "    distinct_bots=('distinct_bots', 'sum')  # Use sum for distinct_bots\n",
    ").reset_index()[:-1] #Getting rid of the last month as it is in complete\n",
    "\n",
    "# Calculate comments per bot per month\n",
    "result_monthly['comments_per_bot'] = result_monthly['videos_commented'] / result_monthly['distinct_bots']\n",
    "\n",
    "# Plot the monthly data\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot videos commented per month\n",
    "ax1.plot(result_monthly['upload_date'], result_monthly['videos_commented'], color='tab:blue', label='Videos Commented (Monthly)', marker='o')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Videos Commented', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Create second y-axis to plot comments per bot per month\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(result_monthly['upload_date'], result_monthly['comments_per_bot'], color='tab:red', label='Comments per Bot (Monthly)', marker='x')\n",
    "ax2.set_ylabel('Comments per Bot', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Bot Comments and Activity Trends (Grouped by Month)')\n",
    "fig.tight_layout()  # Adjust layout to fit everything\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year\n",
    "result_yearly = result.to_pandas().groupby(pd.Grouper(key='upload_date', freq='YS')).agg(\n",
    "    videos_commented=('videos_commented', 'sum'),\n",
    "    distinct_bots=('distinct_bots', 'sum')  # Use sum for distinct_bots\n",
    ").reset_index()[:-1] #Getting rid of the last year as it is in complete\n",
    "\n",
    "# Calculate comments per bot per year\n",
    "result_yearly['comments_per_bot'] = result_yearly['videos_commented'] / result_yearly['distinct_bots']\n",
    "\n",
    "# Plot the yearly data\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot videos commented per year\n",
    "ax1.plot(result_yearly['upload_date'], result_yearly['videos_commented'], color='tab:blue', label='Videos Commented (yearly)', marker='o')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Videos Commented', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Create second y-axis to plot comments per bot per year\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(result_yearly['upload_date'], result_yearly['comments_per_bot'], color='tab:red', label='Comments per Bot (yearly)', marker='x')\n",
    "ax2.set_ylabel('Comments per Bot', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Bot Comments and Activity Trends (Grouped by Year)')\n",
    "fig.tight_layout()  # Adjust layout to fit everything\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **What Is a Typical Lifetime For a Bot ?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1st Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_df = pl.concat([pl.scan_parquet(dataset).group_by(\"author\").agg([\n",
    "        pl.col(\"upload_date\").min().alias(\"min\"),\n",
    "        pl.col(\"upload_date\").max().alias(\"max\")\n",
    "        ]).collect(streaming=True)  \n",
    "    for dataset in tqdm(parquet_files_type_2, desc='Processing df')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that we have the min and max of all users\n",
    "lifetime_df = lifetime_df.group_by(\"author\").agg([\n",
    "                            pl.col(\"min\").min().alias(\"min\"),\n",
    "                            pl.col(\"max\").max().alias(\"max\")\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the lifetime for each accounts and Saves the data\n",
    "\n",
    "date_diff = (lifetime_df['max'] - lifetime_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_df = lifetime_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])\n",
    "\n",
    "lifetime_df.write_parquet('./data/data_type2/lifetime_sus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_df = pl.read_parquet('./data/data_type2/lifetime_sus.parquet')\n",
    "\n",
    "# Extracting the lifetime and converting it to float64 for plotting\n",
    "lifetime = lifetime_df.select(\n",
    "    total_days = pl.col.lifetime.dt.total_days(),\n",
    ")\n",
    "\n",
    "#Plot the distribution of bot lifetimes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lifetime, bins=100, color='blue', edgecolor='black')\n",
    "plt.title(\"Distribution of Bots' Lifetime (in Days)\")\n",
    "plt.xlabel('Lifetime (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2nd Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_sus_df = pl.concat([pl.scan_parquet(dataset).filter(pl.col('videos_commented')>=10).group_by(\"author\")\n",
    "        .agg([\n",
    "        pl.col(\"upload_date\").min().alias(\"min\"),\n",
    "        pl.col(\"upload_date\").max().alias(\"max\")\n",
    "        ]).collect(streaming=True)  \n",
    "    for dataset in tqdm(parquet_files_type_2, desc='Processing df')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that we have the min and max of all users\n",
    "lifetime_sus_df = lifetime_sus_df.group_by(\"author\").agg([\n",
    "                            pl.col(\"min\").min().alias(\"min\"),\n",
    "                            pl.col(\"max\").max().alias(\"max\")\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the lifetime for each account and save the data\n",
    "\n",
    "date_diff = (lifetime_sus_df['max'] - lifetime_sus_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_sus_df = lifetime_sus_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])\n",
    "\n",
    "lifetime_sus_df.write_parquet('./data/data_type2/lifetime_sus_sus_days.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_sus_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_days.parquet')\n",
    "\n",
    "# Extracting the lifetime and converting it to float64 for plotting\n",
    "lifetime = lifetime_sus_df.select(\n",
    "    total_days = pl.col.lifetime.dt.total_days(),\n",
    ")\n",
    "\n",
    "#Plot the distribution of bot lifetimes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lifetime, bins=100, color='blue', edgecolor='black')\n",
    "plt.title(\"Distribution of Bots' suspicious Lifetime (in Days)\")\n",
    "plt.xlabel('Lifetime (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *3rd Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_sus_2_df = lifetime_sus_df.clone()\n",
    "\n",
    "lifetime_sus_2_df = lifetime_sus_2_df.sort(\"author\").with_columns(\n",
    "    max = lifetime_df.sort(\"author\")[\"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the lifetime for each account and save the data\n",
    "\n",
    "date_diff = (lifetime_sus_2_df['max'] - lifetime_sus_2_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_sus_2_df = lifetime_sus_2_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])\n",
    "\n",
    "lifetime_sus_2_df.write_parquet('./data/data_type2/lifetime_sus_sus_2_days.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_sus_2_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_2_days.parquet')\n",
    "\n",
    "# Extracting the lifetime and converting it to float64 for plotting\n",
    "lifetime = lifetime_sus_2_df.select(\n",
    "    total_days = pl.col.lifetime.dt.total_days(),\n",
    ")\n",
    "\n",
    "#Plot the distribution of bot lifetimes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lifetime, bins=100, color='blue', edgecolor='black')\n",
    "plt.title(\"Distribution of Bots' suspicious Lifetime (in Days)\")\n",
    "plt.xlabel('Lifetime (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1st Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'normal'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_normal_type_2 = glob.glob(f'./data/data_type2/{dataset}_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_normal_df = pl.concat([pl.scan_parquet(dataset).group_by(\"author\").agg([\n",
    "        pl.col(\"upload_date\").min().alias(\"min\"),\n",
    "        pl.col(\"upload_date\").max().alias(\"max\")\n",
    "        ]).collect(streaming=True)  \n",
    "    for dataset in tqdm(parquet_normal_type_2, desc='Processing df')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_normal_df = lifetime_normal_df.group_by(\"author\").agg([\n",
    "                            pl.col(\"min\").min().alias(\"min\"),\n",
    "                            pl.col(\"max\").max().alias(\"max\")\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the lifetime for each account and save the data\n",
    "\n",
    "date_diff = (lifetime_normal_df['max'] - lifetime_normal_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_normal_df = lifetime_normal_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])\n",
    "\n",
    "lifetime_normal_df.write_parquet('./data/data_type2/lifetime_normal.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_normal_df = pl.read_parquet('./data/data_type2/lifetime_normal.parquet')\n",
    "\n",
    "# Extracting the lifetime and converting it to float64 for plotting\n",
    "lifetime = lifetime_normal_df.select(\n",
    "    total_days = pl.col.lifetime.dt.total_days(),\n",
    ")\n",
    "\n",
    "#Plot the distribution of bot lifetimes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lifetime, bins=100, color='blue', edgecolor='black')\n",
    "plt.title(\"Distribution of Normal users' Lifetime (in Days)\")\n",
    "plt.xlabel('Lifetime (Days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Comparison Normal vs Bots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1st Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column to discriminate between the 2 df\n",
    "lifetime_df = lifetime_df.with_columns(pl.lit('suspicious').alias(\"category\"))\n",
    "lifetime_normal_df = lifetime_normal_df.with_columns(pl.lit('normal').alias(\"category\"))\n",
    "\n",
    "# Concatenating for the plot\n",
    "df = pl.concat([lifetime_normal_df,lifetime_df]).to_pandas()\n",
    "\n",
    "# Converting datetime to float\n",
    "df[\"lifetime\"] = df[\"lifetime\"]/np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped violinplot\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Changing the order of the first two parts of the palette so normal is blue and sus is red\n",
    "modified_palette = sns.color_palette(\"Pastel1\")\n",
    "modified_palette[0], modified_palette[1] = modified_palette[1], modified_palette[0]\n",
    "\n",
    "sns.violinplot(x=\"category\", y=\"lifetime\", hue=\"category\", data=df[::2], \n",
    "               palette=modified_palette, split=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2nd Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column to discriminate between the 2 df\n",
    "lifetime_sus_df = lifetime_sus_df.with_columns(pl.lit('suspicious').alias(\"category\"))\n",
    "\n",
    "# Concatenating for the plot\n",
    "df = pl.concat([lifetime_normal_df,lifetime_sus_df]).to_pandas()\n",
    "\n",
    "# Converting datetime to float\n",
    "df[\"lifetime\"] = df[\"lifetime\"]/np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped violinplot\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Changing the order of the first two parts of the palette so normal is blue and sus is red\n",
    "modified_palette = sns.color_palette(\"Pastel1\")\n",
    "modified_palette[0], modified_palette[1] = modified_palette[1], modified_palette[0]\n",
    "\n",
    "sns.violinplot(x=\"category\", y=\"lifetime\", hue=\"category\", data=df[::2], \n",
    "               palette=modified_palette, split=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *3rd Definition of Lifetime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column to discriminate between the 2 df\n",
    "lifetime_sus_2_df = lifetime_sus_2_df.with_columns(pl.lit('suspicious').alias(\"category\"))\n",
    "\n",
    "# Concatenating for the plot\n",
    "df = pl.concat([lifetime_normal_df,lifetime_sus_2_df]).to_pandas()\n",
    "\n",
    "# Converting datetime to float\n",
    "df[\"lifetime\"] = df[\"lifetime\"]/np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped violinplot\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Changing the order of the first two parts of the palette so normal is blue and sus is red\n",
    "modified_palette = sns.color_palette(\"Pastel1\")\n",
    "modified_palette[0], modified_palette[1] = modified_palette[1], modified_palette[0]\n",
    "\n",
    "sns.violinplot(x=\"category\", y=\"lifetime\", hue=\"category\", data=df[::2], \n",
    "               palette=modified_palette, split=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Plotly Plots** (Very Heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_df = pl.read_parquet('./data/data_type2/lifetime_sus.parquet')\n",
    "lifetime_normal_df = pl.read_parquet('./data/data_type2/lifetime_normal.parquet')\n",
    "lifetime_sus_2_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_2_days.parquet')\n",
    "lifetime_sus_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_days.parquet')\n",
    "\n",
    "# Adding a column to discriminate between the df\n",
    "lifetime_normal_df = lifetime_normal_df.with_columns(pl.lit('normal').alias(\"category\"))\n",
    "lifetime_df = lifetime_df.with_columns(pl.lit('suspicious1').alias(\"category\"))\n",
    "lifetime_sus_df = lifetime_sus_df.with_columns(pl.lit('suspicious2').alias(\"category\"))\n",
    "lifetime_sus_2_df = lifetime_sus_2_df.with_columns(pl.lit('suspicious3').alias(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.concat([lifetime_df[[\"lifetime\",'category']].with_columns(type_life = 1),\n",
    "           lifetime_normal_df[[\"lifetime\",'category'\n",
    "                               ]].with_columns(\n",
    "                                   type_life = 1).replace_column(\n",
    "                                                             1,pl.Series(\"category\",['normal1']*len(lifetime_normal_df))),\n",
    "           lifetime_sus_df[[\"lifetime\",'category']].with_columns(type_life = 2),\n",
    "           lifetime_normal_df[[\"lifetime\",'category'\n",
    "                               ]].with_columns(\n",
    "                                   type_life = 2).replace_column(\n",
    "                                                             1,pl.Series(\"category\",['normal2']*len(lifetime_normal_df))),\n",
    "           lifetime_sus_2_df[[\"lifetime\",'category']].with_columns(type_life = 3),\n",
    "           lifetime_normal_df[[\"lifetime\",'category'\n",
    "                               ]].with_columns(\n",
    "                                   type_life = 3).replace_column(\n",
    "                                                             1,pl.Series(\"category\",['normal3']*len(lifetime_normal_df))),\n",
    "           ]).to_pandas()[::20]\n",
    "\n",
    "df[\"lifetime\"] = df[\"lifetime\"]/np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=3, shared_yaxes=True)\n",
    "\n",
    "# Add traces to the first subplot\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'normal1'],\n",
    "                        y=df['lifetime'][df['category'] == 'normal1'],\n",
    "                        legendgroup='Yes', scalegroup='normal1', name='Normal',\n",
    "                        side='negative',\n",
    "                        line_color='blue',\n",
    "                        width=0.8),  # Increase width\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'suspicious1'],\n",
    "                        y=df['lifetime'][df['category'] == 'suspicious1'],\n",
    "                        legendgroup='No', scalegroup='normal1', name='Suspicious',\n",
    "                        side='positive',\n",
    "                        line_color='orange',\n",
    "                        width=0.8),  # Increase width\n",
    "              row=1, col=1)\n",
    "\n",
    "# Add traces to the second subplot\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'normal2'],\n",
    "                        y=df['lifetime'][df['category'] == 'normal2'],\n",
    "                        legendgroup='Yes', scalegroup='normal2', name='Normal',\n",
    "                        side='negative',\n",
    "                        line_color='blue',\n",
    "                        width=0.8,\n",
    "                        showlegend=False),  # Increase width\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'suspicious2'],\n",
    "                        y=df['lifetime'][df['category'] == 'suspicious2'],\n",
    "                        legendgroup='No', scalegroup='normal2', name='Suspicious',\n",
    "                        side='positive',\n",
    "                        line_color='orange',\n",
    "                        width=0.8,\n",
    "                        showlegend=False),  # Increase width\n",
    "              row=1, col=2)\n",
    "\n",
    "# Add traces to the third subplot\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'normal3'],\n",
    "                        y=df['lifetime'][df['category'] == 'normal3'],\n",
    "                        legendgroup='Yes', scalegroup='normal3', name='Normal',\n",
    "                        side='negative',\n",
    "                        line_color='blue',\n",
    "                        width=0.8,\n",
    "                        showlegend=False),  # Increase width\n",
    "              row=1, col=3)\n",
    "\n",
    "fig.add_trace(go.Violin(x=df['type_life'][df['category'] == 'suspicious3'],\n",
    "                        y=df['lifetime'][df['category'] == 'suspicious3'],\n",
    "                        legendgroup='No', scalegroup='normal3', name='Suspicious',\n",
    "                        side='positive',\n",
    "                        line_color='orange',\n",
    "                        width=0.8,\n",
    "                        showlegend=False),  # Increase width\n",
    "              row=1, col=3)\n",
    "\n",
    "# Update traces and layout\n",
    "fig.update_traces(meanline_visible=True)\n",
    "fig.update_layout(\n",
    "    violingap=0.1,  # Adjust the gap between violins if needed\n",
    "    violinmode='overlay',\n",
    "    yaxis_title=\"Lifetime\"\n",
    ")\n",
    "\n",
    "# Center the x-axis title for each subplot\n",
    "for i in range(1, 4):\n",
    "    fig.update_xaxes(title_text=\"Type of Lifetime\", title_standoff=20, row=1, col=i)\n",
    "\n",
    "fig.write_html(\"./images/violin_plots_lifetime.html\")\n",
    "#fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Does Commenting Frequency Affects Lifetime ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime_df = pl.read_parquet('./data/data_type2/lifetime_sus.parquet')\n",
    "lifetime_sus_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_days.parquet')\n",
    "lifetime_sus_2_df = pl.read_parquet('./data/data_type2/lifetime_sus_sus_2_days.parquet')\n",
    "numb_app_df = pl.read_parquet('./data/data_type2/numb_sus_days.parquet')\n",
    "\n",
    "lifetime_sus_df = lifetime_sus_df.rename({'lifetime' : 'lifetime_sus'})\n",
    "lifetime_sus_2_df = lifetime_sus_2_df.rename({'lifetime' : 'lifetime_sus_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_effect_df = lifetime_df[['author','lifetime']].join(other = lifetime_sus_df[['author','lifetime_sus']],\n",
    "                                                    on = 'author').join(\n",
    "                                                        other=lifetime_sus_2_df[['author','lifetime_sus_2']],\n",
    "                                                          on ='author'\n",
    "                                                    ).join(other=numb_app_df, on ='author')\n",
    "\n",
    "freq_effect_df = freq_effect_df.rename({'count' : 'numb_day_sus'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the datetime index to float and filtering for the plot\n",
    "lifetime = freq_effect_df.filter(freq_effect_df[\"numb_day_sus\"] <=50).select(\n",
    "    total_days = pl.col.lifetime.dt.total_days(),\n",
    ")\n",
    "\n",
    "lifetime_sus = freq_effect_df.filter(freq_effect_df[\"numb_day_sus\"] <=50).select(\n",
    "    total_days = pl.col.lifetime_sus.dt.total_days(),\n",
    ")\n",
    "\n",
    "lifetime_sus_2 = freq_effect_df.filter(freq_effect_df[\"numb_day_sus\"] <=50).select(\n",
    "    total_days = pl.col.lifetime_sus_2.dt.total_days(),\n",
    ")\n",
    "\n",
    "# Extracting the number of suspicious days\n",
    "numb_day_sus = freq_effect_df.filter(freq_effect_df[\"numb_day_sus\"] <=50)['numb_day_sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D density + marginal distribution:\n",
    "sns.jointplot(x=numb_day_sus[::50], \n",
    "              y=lifetime['total_days'][::50],\n",
    "              cmap=\"Blues\", shade=True, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = freq_effect_df.select(pl.corr('lifetime','numb_day_sus', method='pearson')).item()\n",
    "S = freq_effect_df.select(pl.corr('lifetime','numb_day_sus', method='spearman')).item()\n",
    "\n",
    "print(f\"Pearson's coeff : {P}\")\n",
    "print(f\"Spearman's coeff : {S}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D density + marginal distribution:\n",
    "sns.jointplot(x=numb_day_sus[::50], \n",
    "              y=lifetime_sus['total_days'][::50],\n",
    "              cmap=\"Blues\", shade=True, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = freq_effect_df.select(pl.corr('lifetime_sus','numb_day_sus', method='pearson')).item()\n",
    "S = freq_effect_df.select(pl.corr('lifetime_sus','numb_day_sus', method='spearman')).item()\n",
    "\n",
    "print(f\"Pearson's coeff : {P}\")\n",
    "print(f\"Spearman's coeff : {S}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D density + marginal distribution:\n",
    "sns.jointplot(x=numb_day_sus[::50], \n",
    "              y=lifetime_sus_2['total_days'][::50],\n",
    "              cmap=\"Blues\", shade=True, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = freq_effect_df.select(pl.corr('lifetime_sus_2','numb_day_sus', method='pearson')).item()\n",
    "S = freq_effect_df.select(pl.corr('lifetime_sus_2','numb_day_sus', method='spearman')).item()\n",
    "\n",
    "print(f\"Pearson's coeff : {P}\")\n",
    "print(f\"Spearman's coeff : {S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Do Normal Users' and Bots' Metrics Correlate ?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'suspicious'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_files_type_1 = glob.glob(f'./data/data_type1/{dataset}_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_1_sus = pl.concat([pl.scan_parquet(file).select('author').unique().select(pl.len()).collect() for file in tqdm(parquet_files_type_1)]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sus_type_1=pl.concat([pl.scan_parquet(file) \n",
    "                            for file in parquet_files_type_1])\n",
    "\n",
    "df_sus_type_1 = df_sus_type_1.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_1_sus = 7_586_337\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size_sus = 500_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results_sus = []\n",
    "chunk_results_sus = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start in tqdm(range(0, len_type_1_sus, chunk_size), desc='Processing 1'):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus_type_1.slice(start, chunk_size).collect(streaming=True)\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill_sus = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col_sus in tqdm(columns_to_fill_sus, desc='Filling column'):\n",
    "        if col_sus in chunk_sus.columns:\n",
    "            chunk_sus = chunk_sus.with_columns(pl.col(col_sus).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            #pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            #pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results_sus.append(chunk_metrics_sus)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics_sus = pl.concat(chunk_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start in tqdm(range(0, len_type_1_sus, chunk_size), desc='Processing 2'):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus_type_1.slice(start, chunk_size).collect(streaming=True)\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results_sus.append(unique_users_sus)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined_sus = pl.concat(unique_users_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result_sus = final_metrics_sus.join(unique_users_combined_sus, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result_sus = final_result_sus.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")\n",
    "# Sort the DataFrame by year\n",
    "final_result_sus = final_result_sus.sort(\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'normal'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_files_type_1 = glob.glob(f'./data/data_type1/{dataset}_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_1_normal = pl.concat([pl.scan_parquet(file).select('author').unique().select(pl.len()).collect() for file in tqdm(parquet_files_type_1)]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_type_1=pl.concat([pl.scan_parquet(file) \n",
    "                            for file in parquet_files_type_1])\n",
    "\n",
    "df_normal_type_1 = df_normal_type_1.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_1_normal = 5_287_367\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1_000_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results = []\n",
    "chunk_results = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start in tqdm(range(0, len_type_1_normal, chunk_size), desc='Processing 1'):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = df_normal_type_1.slice(start, chunk_size).collect(streaming=True)\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col in tqdm(columns_to_fill, desc='Filling column'):\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk.with_columns(pl.col(col).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            #pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            #pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results.append(chunk_metrics)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics = pl.concat(chunk_results).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start in tqdm(range(0, len_type_1_normal, chunk_size), desc='Processing 2'):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = df_normal_type_1.slice(start, chunk_size).collect(streaming=True)\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results.append(unique_users)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined = pl.concat(unique_users_results).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result = final_metrics.join(unique_users_combined, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result = final_result.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Polars DataFrames to Pandas DataFrames\n",
    "df1_pd = final_result.to_pandas()\n",
    "df2_pd = final_result_sus.to_pandas()\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "concatenated_df = pd.concat([df1_pd, df2_pd], axis=1, keys=[\"normal\", \"sus\"])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = concatenated_df.corr().abs()\n",
    "\n",
    "# Get the number of columns\n",
    "num_cols = len(correlation_matrix.columns)\n",
    "\n",
    "# Slice the correlation matrix to get the top right quarter\n",
    "# This will select the upper right quarter of the matrix\n",
    "top_right_quarter = correlation_matrix.iloc[:num_cols//2, num_cols//2:]\n",
    "\n",
    "# Plot the sliced correlation matrix using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(top_right_quarter, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Top Right Quarter Correlation Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'year' column to a date type (assuming the 'year' contains year only, adjust if needed)\n",
    "df = final_result.with_columns(pl.col(\"year\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y\"))\n",
    "\n",
    "# Extract the 'year' column as a pandas DataFrame\n",
    "date_column = df.select(\"year\").to_pandas()\n",
    "\n",
    "# Drop the 'year' column from the DataFrame for scaling\n",
    "df_without_date = df.drop(\"year\").to_pandas()\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_array = scaler.fit_transform(df_without_date)\n",
    "\n",
    "# Convert the scaled array back to a Pandas DataFrame\n",
    "scaled_df_pd = pd.DataFrame(scaled_array, columns=df_without_date.columns)\n",
    "\n",
    "# Add the 'year' column back to the scaled DataFrame\n",
    "scaled_df_pd[\"year\"] = date_column\n",
    "\n",
    "# Convert the Pandas DataFrame back to a Polars DataFrame\n",
    "scaled_df = pl.DataFrame(scaled_df_pd)\n",
    "\n",
    "final_result = scaled_df.sort('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'year' column to a date type (assuming the 'year' contains year only, adjust if needed)\n",
    "df = final_result_sus.with_columns(pl.col(\"year\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y\"))\n",
    "\n",
    "# Extract the 'year' column as a pandas DataFrame\n",
    "date_column = df.select(\"year\").to_pandas()\n",
    "\n",
    "# Drop the 'year' column from the DataFrame for scaling\n",
    "df_without_date = df.drop(\"year\").to_pandas()\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_array = scaler.fit_transform(df_without_date)\n",
    "\n",
    "# Convert the scaled array back to a Pandas DataFrame\n",
    "scaled_df_pd = pd.DataFrame(scaled_array, columns=df_without_date.columns)\n",
    "\n",
    "# Add the 'year' column back to the scaled DataFrame\n",
    "scaled_df_pd[\"year\"] = date_column\n",
    "\n",
    "# Convert the Pandas DataFrame back to a Polars DataFrame\n",
    "scaled_df = pl.DataFrame(scaled_df_pd)\n",
    "\n",
    "final_result_sus = scaled_df.sort('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Polars DataFrames to Pandas DataFrames\n",
    "df1_pd = final_result.to_pandas()\n",
    "df2_pd = final_result_sus.to_pandas()\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "concatenated_df = pd.concat([df1_pd, df2_pd], axis=1, keys=[\"normal\", \"sus\"])\n",
    "\n",
    "# Reset the index to flatten the hierarchical index\n",
    "concatenated_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in concatenated_df.columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = concatenated_df.corr().abs()\n",
    "\n",
    "# Get the number of columns\n",
    "num_cols = len(correlation_matrix.columns)\n",
    "\n",
    "# Slice the correlation matrix to get the top right quarter\n",
    "# This will select the upper right quarter of the matrix\n",
    "top_right_quarter = correlation_matrix.iloc[:num_cols//2-1, num_cols//2:-1]\n",
    "\n",
    "# Plot the sliced correlation matrix using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(top_right_quarter, annot=True, cmap='coolwarm', fmt='.2f', vmin=0, vmax=1)\n",
    "plt.title(\"Correlation map of the evolution of normal and suspicious users\")\n",
    "plt.savefig('./images/corr_map_evol_normal_sus_T1.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=top_right_quarter.values,\n",
    "    x=top_right_quarter.columns,\n",
    "    y=top_right_quarter.index,\n",
    "    colorscale=px.colors.diverging.RdBu[::-1],\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    text = top_right_quarter.round(2).values,\n",
    "    texttemplate=\"%{text}\", hovertemplate=None\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_nticks=36,\n",
    "    xaxis_title='Suspicious',\n",
    "    yaxis_title='Normal' \n",
    ")\n",
    "fig.write_html(\"./images/heatmap_norm_sus.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center\"> <span style=\"text-decoration: underline\"> **Intersection of Type-1 and 2 Bot Analysis** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'suspicious'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_files_type_1 = glob.glob(f'./data/data_type1/{dataset}_*.parquet')\n",
    "parquet_files_type_2 = glob.glob(f'./data/data_type2/{dataset}_*.parquet')\n",
    "\n",
    "# Read and concatenate all author columns of Parquet files\n",
    "authors_type_1 = pl.concat([pl.scan_parquet(file).select(\"author\").collect() \n",
    "                            for file in parquet_files_type_1]).unique()\n",
    "\n",
    "authors_type_2 = pl.concat([pl.scan_parquet(file).select('author').unique()\n",
    "                            for file in parquet_files_type_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_2 = pl.concat([pl.scan_parquet(file).select('author').unique().select(pl.len()).collect() for file in parquet_files_type_2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1_000_000\n",
    "\n",
    "# Setting the DF for both type of suspicious accounts\n",
    "authors_1_2 = pl.DataFrame()\n",
    "\n",
    "for start in tqdm(range(0, len_type_2, chunk_size), desc=\"Processing chunks\"):\n",
    "\n",
    "    chunk = authors_type_2.slice(start, chunk_size).collect()\n",
    "\n",
    "\t# Only keeping the unique ID to save RAM\n",
    "    joined_chunk = authors_type_1.join(chunk, on=\"author\", how=\"inner\").unique()\n",
    "\n",
    "    authors_1_2 = pl.concat([authors_1_2, joined_chunk])\n",
    "\n",
    "authors_1_2 = authors_1_2.unique()\n",
    "\n",
    "# Save the data\n",
    "authors_1_2.write_parquet(\"./data/intersection/suspicious_users_1_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "authors_1_2 = pl.read_parquet(\"./data/intersection/suspicious_users_1_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_type_2 = 3845594\n",
    "len_type_1 = len(authors_type_1.unique())\n",
    "len_1_2 = len(authors_1_2)\n",
    "\n",
    "print(f\"Ratio of users in type 1 : {len_1_2/len_type_1*100}\")\n",
    "print(f\"Ratio of users in type 2 : {len_1_2/len_type_2*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read partial Parquet files & create a single df\n",
    "\n",
    "#dataset = 'normal_users'\n",
    "dataset = 'suspicious'\n",
    "\n",
    "print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "# List all Parquet files\n",
    "parquet_files_type_1 = glob.glob(f'./data/data_type1/{dataset}_*.parquet')\n",
    "parquet_files_type_2 = glob.glob(f'./data/data_type2/{dataset}_*.parquet')\n",
    "\n",
    "# Read and concatenate all author columns of Parquet files\n",
    "authors_type_1 = pl.concat([pl.scan_parquet(file).select([\"author\",\"upload_date\"]).collect() \n",
    "                            for file in parquet_files_type_1])\n",
    "\n",
    "authors_type_2 = pl.concat([pl.scan_parquet(file).select(['author','upload_date'])\n",
    "                            for file in parquet_files_type_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_1_2_date = authors_type_1.join(other=authors_1_2, on=\"author\", how=\"inner\")\n",
    "authors_1_excl = authors_type_1.join(other=authors_1_2, on=\"author\", how=\"anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the unique type 2 accounts\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "# Setting the DF for both type of suspicious accounts\n",
    "authors_2_excl = pl.DataFrame()\n",
    "\n",
    "for start in tqdm(range(0, len_type_2, chunk_size), desc=\"Processing chunks\"):\n",
    "\n",
    "    chunk = authors_type_2.slice(start, chunk_size).collect()\n",
    "\n",
    "\t# Only keeping the authors that are only type 2\n",
    "    anti_joined_chunk = chunk.join(other=authors_1_2, on=\"author\", how=\"anti\")\n",
    "\n",
    "    authors_2_excl = pl.concat([authors_2_excl, anti_joined_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_1_excl = authors_1_excl.with_columns([\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"upload_date\").dt.month().alias(\"month\")\n",
    "])\n",
    "\n",
    "authors_2_excl = authors_2_excl.with_columns([\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"upload_date\").dt.month().alias(\"month\")\n",
    "])\n",
    "\n",
    "authors_1_2_date = authors_1_2_date.with_columns([\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"upload_date\").dt.month().alias(\"month\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and month and sum the values\n",
    "authors_1_excl = authors_1_excl.group_by([\"year\", \"month\"]).agg([\n",
    "    pl.count()\n",
    "])\n",
    "\n",
    "authors_2_excl = authors_2_excl.group_by([\"year\", \"month\"]).agg([\n",
    "    pl.count()\n",
    "])\n",
    "\n",
    "authors_1_2_date = authors_1_2_date.group_by([\"year\", \"month\"]).agg([\n",
    "    pl.count()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pandas df each type before concatenating them\n",
    "# Replacing the columns year month by a unique datetime column\n",
    "authors_1_excl = authors_1_excl.with_columns([\n",
    "    (pl.col(\"year\") * 100 + pl.col(\"month\")).cast(pl.Utf8).alias(\"year_month\")\n",
    "]).sort(\"year_month\")\n",
    "\n",
    "authors_2_excl = authors_2_excl.with_columns([\n",
    "    (pl.col(\"year\") * 100 + pl.col(\"month\")).cast(pl.Utf8).alias(\"year_month\")\n",
    "]).sort(\"year_month\")\n",
    "\n",
    "authors_1_2_date = authors_1_2_date.with_columns([\n",
    "    (pl.col(\"year\") * 100 + pl.col(\"month\")).cast(pl.Utf8).alias(\"year_month\")\n",
    "]).sort(\"year_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the useless columns\n",
    "authors_1_excl = authors_1_excl.drop([\"year\",'month'])\n",
    "authors_2_excl = authors_2_excl.drop([\"year\",'month'])\n",
    "authors_1_2_date = authors_1_2_date.drop([\"year\",'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas\n",
    "authors_1_excl = authors_1_excl.to_pandas()\n",
    "authors_2_excl = authors_2_excl.to_pandas()\n",
    "authors_1_2_date = authors_1_2_date.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year_month' to datetime format\n",
    "authors_1_excl['year_month'] = authors_1_excl['year_month'].apply(lambda x: datetime.strptime(x, \"%Y%m\"))\n",
    "authors_2_excl['year_month'] = authors_2_excl['year_month'].apply(lambda x: datetime.strptime(x, \"%Y%m\"))\n",
    "authors_1_2_date['year_month'] = authors_1_2_date['year_month'].apply(lambda x: datetime.strptime(x, \"%Y%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_1_excl = authors_1_excl.set_index('year_month')\n",
    "authors_2_excl = authors_2_excl.set_index('year_month')\n",
    "authors_1_2_date = authors_1_2_date.set_index('year_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_1_excl = authors_1_excl.rename(columns={\"count\":'authors_1_excl'})\n",
    "authors_2_excl = authors_2_excl.rename(columns={\"count\":'authors_2_excl'})\n",
    "authors_1_2_date = authors_1_2_date.rename(columns={\"count\":'authors_1_2'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new df for the stacked area plot | Deleting last month because not complete\n",
    "authors_evol_df = pd.concat([authors_1_excl,authors_2_excl,authors_1_2_date], axis=1).fillna(0)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the stacked area chart with smoothing and custom colors\n",
    "plt.figure(figsize=(6, 6))  # Set the figure size\n",
    "plt.stackplot(authors_evol_df.index,\n",
    "              authors_evol_df.values.T,\n",
    "              labels=authors_evol_df.columns)\n",
    "plt.xlabel('Year') # Add a label for the x-axis\n",
    "plt.ylabel('Total number of suspicious accounts') # Add a label for the y-axis\n",
    "plt.title('Stacked Area Chart of the different type of bots') # Add a title\n",
    "plt.legend(loc='upper left') # Add a legend in the upper left corner of the plot\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for the countries\n",
    "custom_colors = [\"#003f5c\",\"#2f4b7c\",\"#665191\",\"#a05195\",\"#d45087\",\"#f95d6a\",\"#ff7c43\",\"#ffa600\"]\n",
    "\n",
    "custom_colors = [\"#d45087\",\"#f95d6a\",\"#ffa600\"]\n",
    "\n",
    "# Define the desired order of countries\n",
    "desired_order = [\"authors_1_2\", \"authors_1_excl\", \"authors_2_excl\"]\n",
    "\n",
    "# Reorder the columns of the pivot_df and custom_colors list\n",
    "authors_evol_df = authors_evol_df[desired_order]\n",
    "\n",
    "\n",
    "# Convert the datetime index to numeric values (nanoseconds since the Unix epoch)\n",
    "x_numeric = authors_evol_df.index.astype(np.int64)\n",
    "\n",
    "# Smooth the lines using spline interpolation\n",
    "x_smooth_numeric = np.linspace(x_numeric.min(), x_numeric.max(), 300)\n",
    "pivot_smooth = pd.DataFrame({author_type: make_interp_spline(x_numeric, authors_evol_df[author_type])(x_smooth_numeric)\n",
    "                             for author_type in authors_evol_df.columns})\n",
    "\n",
    "# Convert the smoothed numeric values back to datetime\n",
    "x_smooth = pd.to_datetime(x_smooth_numeric)\n",
    "\n",
    "# Plot the stacked area chart with smoothing and custom colors\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.stackplot(x_smooth,\n",
    "              pivot_smooth.values.T,\n",
    "              labels=pivot_smooth.columns,\n",
    "              colors=custom_colors)\n",
    "plt.xlabel('Year') # Add a label for the x-axis\n",
    "plt.ylabel('Total number of suspicious accounts') # Add a label for the y-axis\n",
    "#plt.yscale(\"symlog\")\n",
    "plt.title('Stacked Area Chart of the different type of bots') # Add a title\n",
    "plt.legend(loc='upper left') # Add a legend in the upper left corner of the plot\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ---------\n",
    "#### Smoothing the graph\n",
    "# Define custom colors for the countries\n",
    "custom_colors = [\"#d45087\", \"#f95d6a\", \"#ffa600\"]\n",
    "\n",
    "# Define the desired order of countries\n",
    "desired_order = [\"authors_1_2\", \"authors_1_excl\", \"authors_2_excl\"]\n",
    "\n",
    "# Reorder the columns of the pivot_df and custom_colors list\n",
    "authors_evol_df = authors_evol_df[desired_order]\n",
    "\n",
    "# Convert the datetime index to numeric values (nanoseconds since the Unix epoch)\n",
    "x_numeric = authors_evol_df.index.astype(np.int64)\n",
    "\n",
    "# Smooth the lines using spline interpolation\n",
    "x_smooth_numeric = np.linspace(x_numeric.min(), x_numeric.max(), 300)\n",
    "pivot_smooth = pd.DataFrame({author_type: make_interp_spline(x_numeric, authors_evol_df[author_type])(x_smooth_numeric)\n",
    "                             for author_type in authors_evol_df.columns})\n",
    "\n",
    "# Convert the smoothed numeric values back to datetime\n",
    "x_smooth = pd.to_datetime(x_smooth_numeric)\n",
    "\n",
    "### -------\n",
    "### Getting the limits\n",
    "start_time =authors_evol_df.index[0] - pd.Timedelta(weeks=4)\n",
    "end_time = authors_evol_df.index[-1] +  pd.Timedelta(weeks=4)\n",
    "\n",
    "### -------\n",
    "### Making the plot\n",
    "\n",
    "# Plot the stacked area chart with smoothing and custom colors\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size\n",
    "plt.stackplot(x_smooth, pivot_smooth.values.T, labels=pivot_smooth.columns, colors=custom_colors)\n",
    "\n",
    "# Allowing latex for equations\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# `plt.gca()` function is used to obtain a reference to the current axes on which you plot your data\n",
    "ax = plt.gca()\n",
    "\n",
    "# Annotations for the values per year\n",
    "def add_annotations_year(date):\n",
    "    \"\"\"\n",
    "    Input: a year\n",
    "    Apply: add to the graph the total wealth of all countries at a given date\n",
    "           and a line from the bottom of the graph to the total value of wealth\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate total amount of wealth at a given year\n",
    "    y_end = int(authors_evol_df.loc[date].sum())\n",
    "    \n",
    "    # Set values in areas where the graph does not appear\n",
    "    # Special case for 2021: we put it on the left instead of upper the line\n",
    "    if date==authors_evol_df.index[-1]:\n",
    "        date_text = date + pd.Timedelta(weeks=6)\n",
    "        modif_yaxis = 20000\n",
    "    else:\n",
    "        date_text = date - pd.Timedelta(weeks=22)\n",
    "        modif_yaxis = 20000\n",
    "    \n",
    "    # Add the values, with a specific position, in bold, black and a fontsize of 10\n",
    "    plt.text(date_text,\n",
    "             y_end+modif_yaxis,\n",
    "             f'{y_end}',\n",
    "             fontsize=15,\n",
    "             color='black',\n",
    "             fontweight = 'bold')\n",
    "    \n",
    "    # Add line \n",
    "    ax.plot([date, date], # x-axis position\n",
    "            [0, y_end*1.05], # y-axis position (*1.05 is used to make a it little bit longer)\n",
    "            color='black', # Color\n",
    "            linewidth=1.3) # Width of the line\n",
    "    \n",
    "    # Add a point at the top of the line\n",
    "    ax.plot(date, # x-axis position\n",
    "            y_end*1.05, # y-axis position (*1.05 is used to make a it little bit longer)\n",
    "            marker='o', # Style of the point\n",
    "            markersize=5, # Size of the point\n",
    "            color='black') # Color\n",
    "\n",
    "# Add the line and the values for each of the following years\n",
    "for date in ['2008-01-01', '2011-05-01','2013-08-01','2017-01-01','2019-09-01']:\n",
    "    add_annotations_year(pd.Timestamp(date))\n",
    "    \n",
    "\n",
    "# Annotations for the values per country\n",
    "def add_annotations_country(country, value_placement, amount, color):\n",
    "    \"\"\"\n",
    "    Adds an annotation to a plot at a specific location with information about a country's amount in millions.\n",
    "\n",
    "    Parameters:\n",
    "        country (str): The name of the country for which the annotation is being added.\n",
    "        value_placement (float): The vertical position where the annotation will be placed on the plot.\n",
    "        amount (float): The amount in millions that will be displayed in the annotation.\n",
    "        color (str): The color of the annotation text.\n",
    "    \"\"\"\n",
    "    plt.text(pd.Timestamp('2020-01-01'), value_placement, f'{country} : {amount}', fontsize=15, color=color, fontweight='bold')\n",
    "\n",
    "# We manually define the labels, values and position that will be displayed on the right of the graph\n",
    "countries = [r'$\\overline{T}_2$', r'$\\overline{T}_1$', r\"$T_{1,2}$\"]\n",
    "values_placement = [430000, 400000, 295000]\n",
    "amounts = [int(authors_evol_df[\"authors_2_excl\"][-1]), int(authors_evol_df[\"authors_1_excl\"][-1]),\n",
    "            int(authors_evol_df[\"authors_1_2\"][-1])]\n",
    "custom_colors.reverse()  # Makes sure the colors match the country concerned\n",
    "\n",
    "# Iterate over all countries and add the name with the right value and color\n",
    "for country, value, amount, color in zip(countries, values_placement, amounts, custom_colors):\n",
    "    add_annotations_country(country, value, amount, color)\n",
    "\n",
    "\n",
    "# Remove the y-axis frame (left, right and top spines)\n",
    "#ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Remove the ticks and labels on the y-axis\n",
    "#ax.tick_params(left=False, labelleft=False)\n",
    "# Grid on the y-axis \n",
    "plt.grid(axis=\"y\", zorder=-2, linestyle='--', color='gray', alpha=0.5) \n",
    "\n",
    "# Set the x-axis limits to ensure the text and plot fit well\n",
    "plt.xlim(start_time, end_time)\n",
    "\n",
    "# Set the y-axis limits to ensure the text and plot fit well\n",
    "plt.ylim(0, 500000)\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(\"./images/different_types_of_bots.svg\", bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **BOT RANKING** </span> </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Type-1 Bot Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lifetime of suspicious users\n",
    "\n",
    "lifetime_df = df_sus.group_by(\"author\").agg([\n",
    "                pl.col(\"upload_date\").min().alias(\"min\"),\n",
    "                pl.col(\"upload_date\").max().alias(\"max\")])\n",
    "\n",
    "# Compute the difference\n",
    "date_diff = (lifetime_df['max'] - lifetime_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_df = lifetime_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longuest_survival_type1=lifetime_df[\"lifetime\"].max()\n",
    "longest_survival_row_type1 = lifetime_df.filter(pl.col(\"lifetime\") == pl.col(\"lifetime\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total comments for each unique author across every day\n",
    "total_comments_df = df_sus.group_by(\"author\").agg([\n",
    "    pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "])\n",
    "\n",
    "# Max number of comments done in a day for each author\n",
    "max_daily_comments_df = df_sus.group_by(\"author\").agg([\n",
    "    pl.col(\"comments\").max().alias(\"max_daily_comments\"),\n",
    "])\n",
    "\n",
    "# Combine the two metrics into a single DataFrame\n",
    "author_metrics_df = total_comments_df.join(max_daily_comments_df, on=\"author\")\n",
    "\n",
    "# Extract the row with the maximum total comments\n",
    "row_with_max_total_comments = author_metrics_df.filter(pl.col(\"total_comments\") == pl.col(\"total_comments\").max())\n",
    "\n",
    "# Extract the row with the maximum daily comments\n",
    "row_with_max_daily_comments = df_sus.filter(pl.col(\"comments\") == pl.col(\"comments\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Max number of different videos commented by an author in a day\n",
    "max_videos_per_day = (\n",
    "    df_sus.group_by([\"author\", \"upload_date\"])\n",
    "    .agg([\n",
    "        pl.col(\"video_id\").n_unique().alias(\"unique_videos_commented\")\n",
    "    ])\n",
    "    .group_by(\"author\")\n",
    "    .agg([\n",
    "        pl.col(\"unique_videos_commented\").max().alias(\"max_videos_in_a_day\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 2. Max number of different videos commented across the entire dataset for each author\n",
    "max_videos_overall = (\n",
    "    df_sus.group_by(\"author\")\n",
    "    .agg([\n",
    "        pl.col(\"video_id\").n_unique().alias(\"max_videos_overall\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Retrieve the row with the max number of different videos commented in a day\n",
    "row_with_max_videos_in_a_day = max_videos_per_day.filter(\n",
    "    pl.col(\"max_videos_in_a_day\") == pl.col(\"max_videos_in_a_day\").max()\n",
    ")\n",
    "\n",
    "# Retrieve the row with the max number of different videos commented across the entire dataset\n",
    "row_with_max_videos_overall = max_videos_overall.filter(\n",
    "    pl.col(\"max_videos_overall\") == pl.col(\"max_videos_overall\").max()\n",
    ")\n",
    " \n",
    "max_videos_per_day.filter(pl.col(\"max_videos_in_a_day\")== pl.col(\"max_videos_in_a_day\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter 1: Count days with more than 10 comments per day\n",
    "filter_1_days = (\n",
    "    df_sus.filter(pl.col(\"comments\") > 10)\n",
    "    .group_by(\"author\")\n",
    "    .agg(pl.col(\"upload_date\").n_unique().alias(\"suspicious_days_filter_1\"))\n",
    ")\n",
    "\n",
    "# 2. Filter 2: Count days with more than 10 unique videos commented\n",
    "filter_2_days = (\n",
    "    df_sus.group_by([\"author\", \"upload_date\"])\n",
    "    .agg(pl.col(\"video_id\").n_unique().alias(\"unique_videos_commented\"))\n",
    "    .filter(pl.col(\"unique_videos_commented\") > 10)\n",
    "    .group_by(\"author\")\n",
    "    .agg(pl.col(\"upload_date\").n_unique().alias(\"suspicious_days_filter_2\"))\n",
    ")\n",
    "\n",
    "# # 3. Combine results into one DataFrame\n",
    "# suspicious_lifetime = filter_1_days.join(filter_2_days, on=\"author\", how=\"outer\").fill_null(0)\n",
    "\n",
    "# 4. Extract rows with maximum suspicious days for each filter\n",
    "max_suspicious_days_filter_1 = filter_1_days.filter(\n",
    "    pl.col(\"suspicious_days_filter_1\") == pl.col(\"suspicious_days_filter_1\").max()\n",
    ")\n",
    "max_suspicious_days_filter_2 = filter_2_days.filter(\n",
    "    pl.col(\"suspicious_days_filter_2\") == pl.col(\"suspicious_days_filter_2\").max()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Type-2 Bot Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sus_type_2=pl.read_parquet('data/data_type2/combi_suspicious_dataset2.parquet')\n",
    "\n",
    "lifetime_df_type2 = df_sus_type_2.group_by(\"author\").agg([\n",
    "                pl.col(\"upload_date\").filter(pl.col('videos_commented')>=10).min().alias(\"min\"),\n",
    "                pl.col(\"upload_date\").max().alias(\"max\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Lifetime\n",
    "\n",
    "date_diff_type2 = (lifetime_df_type2['max'] - lifetime_df_type2['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_df_type2 = lifetime_df_type2.with_columns([\n",
    "    (date_diff_type2 + one_day).alias(\"lifetime\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longuest_survival_type2=lifetime_df_type2[\"lifetime\"].max()\n",
    "longest_survival_row_type2 = lifetime_df_type2.filter(pl.col(\"lifetime\") == pl.col(\"lifetime\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_1_2=pl.read_parquet('data/suspicious_users_1_2.parquet')\n",
    "recup_type_2= inter_1_2.join(df_sus, on=\"author\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter 1: Count days with more than 10 comments per day\n",
    "max_suspicious_days_filter_1_type2 = (\n",
    "    recup_type_2.filter(pl.col(\"comments\") > 10)\n",
    "    .group_by(\"author\")\n",
    "    .agg(pl.col(\"upload_date\").n_unique().alias(\"suspicious_days_filter_1\"))\n",
    "    .filter(pl.col(\"suspicious_days_filter_1\") == pl.col(\"suspicious_days_filter_1\").max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type2_on_type2=recup_type_2.filter(pl.col(\"comments\") > 10).group_by(\"author\").agg(pl.col(\"upload_date\").n_unique().alias(\"suspicious_days_filter_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_app_df = pl.concat([pl.scan_parquet(dataset).filter(pl.col('videos_commented') >=10).group_by('author').count().collect(streaming=True)  \n",
    "    for dataset in tqdm(parquet_files_type_2, desc='Processing df')\n",
    "    ]).group_by('author').agg(pl.col(\"count\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the row(s) with the maximum \"count\" meaning number of suspicious days in the type2 dataset\n",
    "max_count_value = numb_app_df[\"count\"].max()\n",
    "\n",
    "# Retrieve the row(s) with the maximum count\n",
    "rows_with_max_count = numb_app_df.filter(pl.col(\"count\") == max_count_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total comments for each unique author across every day\n",
    "total_comments_df_type_2 = recup_type_2.group_by(\"author\").agg([\n",
    "    pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "])\n",
    "\n",
    "# Max number of comments done in a day for each author\n",
    "max_daily_comments_df_type2 = recup_type_2.group_by(\"author\").agg([\n",
    "    pl.col(\"comments\").max().alias(\"max_daily_comments\"),\n",
    "])\n",
    "\n",
    "# Combine the two metrics into a single DataFrame\n",
    "author_metrics_df_type_2 = total_comments_df_type_2.join(max_daily_comments_df_type2, on=\"author\")\n",
    "\n",
    "# Extract the row with the maximum total comments\n",
    "row_with_max_total_comments_type_2 = author_metrics_df_type_2.filter(pl.col(\"total_comments\") == pl.col(\"total_comments\").max())\n",
    "\n",
    "# Extract the row with the maximum daily comments\n",
    "row_with_max_daily_comments_type_2 = recup_type_2.filter(pl.col(\"comments\") == pl.col(\"comments\").max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Max number of different videos commented across the entire dataset for each author\n",
    "max_videos_overall_type_2 = df_sus_type_2.group_by(\"author\").agg(pl.col(\"videos_commented\").sum().alias(\"max_videos_overall\"))\n",
    "\n",
    "# Retrieve the row with the max number of different videos commented in a day\n",
    "row_with_max_videos_in_a_day_type_2 = df_sus_type_2[\"videos_commented\"].max()\n",
    "\n",
    "# Retrieve the row with the max number of different videos commented across the entire dataset\n",
    "row_with_max_videos_overall = max_videos_overall_type_2[\"max_videos_overall\"].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Retrieve the row with the max number of videos commented in a day\n",
    "row_with_max_videos_in_a_day_type_2 = df_sus_type_2.filter(\n",
    "    pl.col(\"videos_commented\") == pl.col(\"videos_commented\").max()\n",
    ")\n",
    "\n",
    "# 3. Retrieve the row with the max number of videos commented across the entire dataset\n",
    "row_with_max_videos_overall = max_videos_overall_type_2.filter(\n",
    "    pl.col(\"max_videos_overall\") == pl.col(\"max_videos_overall\").max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving all the 7 metrics for each top value ! \n",
    "            #champ1     #champ2     #champ3  #champ4\n",
    "champions= [551221769, 260739815, 157898414, 303364156]\n",
    "\n",
    "#Retrieve the lifetime: \n",
    "champ1_lifetime= lifetime_df.filter(pl.col(\"author\") == 551221769)\n",
    "print(champ1_lifetime)\n",
    "\n",
    "champ1_lifetime_t2= lifetime_df_type2.filter(pl.col(\"author\") == 551221769)\n",
    "print(champ1_lifetime_t2)\n",
    "#retrieve max comm in a day: \n",
    "row_with_max_daily_comments_champ1 = df_sus.filter(pl.col(\"author\") == 551221769).filter(pl.col(\"comments\") == pl.col(\"comments\").max())\n",
    "print(row_with_max_daily_comments_champ1)\n",
    "\n",
    "#retrieve total number of comm: \n",
    "total_com_champ1=total_comments_df.filter(pl.col(\"author\") == 551221769)\n",
    "print(total_com_champ1)\n",
    "\n",
    "#retrieve max # of different videos in a day\n",
    "max_videos_per_day_champ1=max_videos_per_day.filter(pl.col(\"author\") == 551221769).filter(pl.col(\"max_videos_in_a_day\")== pl.col(\"max_videos_in_a_day\").max())\n",
    "print(max_videos_per_day_champ1)\n",
    "\n",
    "#retrieve max # of diff videos in total\n",
    "max_videos_overall_champ1=max_videos_overall.filter(pl.col(\"author\") == 551221769)\n",
    "print(max_videos_overall_champ1)\n",
    "\n",
    "#retrieve number of suspicious day type 1\n",
    "suspicious_daysf1_champ1=filter_1_days.filter(pl.col(\"author\") == 551221769)\n",
    "print(suspicious_daysf1_champ1)\n",
    "\n",
    "    #If in type2: \n",
    "suspicious_daysf1_t2_champ1=filter_type2_on_type2.filter(pl.col(\"author\") == 551221769)\n",
    "print(suspicious_daysf1_t2_champ1)\n",
    "\n",
    "#retrieve number of suspicious day type 2 \n",
    "suspicious_daysf2_champ1=filter_2_days.filter(pl.col(\"author\") == 551221769)\n",
    "print(suspicious_daysf2_champ1)\n",
    "    #If in type2:\n",
    "suspicious_daysf2_t2_champ1=numb_app_df.filter(pl.col(\"author\") == 551221769)\n",
    "print(suspicious_daysf2_t2_champ1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champions list\n",
    "champions = [551221769, 260739815, 157898414, 303364156]\n",
    "\n",
    "# Loop through each champion\n",
    "for champ in champions:\n",
    "    print(f\"\\n=== Retrieving metrics for Champion: {champ} ===\")\n",
    "    \n",
    "    # Retrieve lifetime\n",
    "    champ_lifetime = lifetime_df.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Lifetime (Overall):\")\n",
    "    print(champ_lifetime)\n",
    "    \n",
    "    champ_lifetime_t2 = lifetime_df_type2.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Lifetime (Type 2):\")\n",
    "    print(champ_lifetime_t2)\n",
    "    \n",
    "    # Retrieve max comments in a day\n",
    "    row_with_max_daily_comments = df_sus.filter(pl.col(\"author\") == champ).filter(\n",
    "        pl.col(\"comments\") == pl.col(\"comments\").max()\n",
    "    )\n",
    "    print(\"Max Comments in a Day:\")\n",
    "    print(row_with_max_daily_comments)\n",
    "    \n",
    "    # Retrieve total number of comments\n",
    "    total_com = total_comments_df.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Total Comments:\")\n",
    "    print(total_com)\n",
    "    \n",
    "    # Retrieve max # of different videos in a day\n",
    "    max_videos_per_day_champ = max_videos_per_day.filter(pl.col(\"author\") == champ).filter(\n",
    "        pl.col(\"max_videos_in_a_day\") == pl.col(\"max_videos_in_a_day\").max()\n",
    "    )\n",
    "    print(\"Max Different Videos in a Day:\")\n",
    "    print(max_videos_per_day_champ)\n",
    "    \n",
    "    # Retrieve max # of different videos overall\n",
    "    max_videos_overall = max_videos_overall.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Max Different Videos Overall:\")\n",
    "    print(max_videos_overall)\n",
    "    \n",
    "    # Retrieve max # of different videos overall in data from type 2\n",
    "    max_videos_overall_type2=max_videos_overall_type_2.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Max Different Videos Overall: (type2)\")\n",
    "    print(max_videos_overall_type2)\n",
    "\n",
    "    # Retrieve number of suspicious days (Filter 1)\n",
    "    suspicious_daysf1 = filter_1_days.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Suspicious Days (Filter 1):\")\n",
    "    print(suspicious_daysf1)\n",
    "    \n",
    "    # Retrieve number of suspicious days (Filter 1, Type 2)\n",
    "    suspicious_daysf1_t2 = filter_type2_on_type2.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Suspicious Days (Filter 1, Type 2):\")\n",
    "    print(suspicious_daysf1_t2)\n",
    "    \n",
    "    # Retrieve number of suspicious days (Filter 2)\n",
    "    suspicious_daysf2 = filter_2_days.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Suspicious Days (Filter 2):\")\n",
    "    print(suspicious_daysf2)\n",
    "    \n",
    "    # Retrieve number of suspicious days (Filter 2, Type 2)\n",
    "    suspicious_daysf2_t2 = numb_app_df.filter(pl.col(\"author\") == champ)\n",
    "    print(\"Suspicious Days (Filter 2, Type 2):\")\n",
    "    print(suspicious_daysf2_t2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data\n",
    "champion_metrics = {\n",
    "    \"group\": ['C-3PO', 'T-800', 'HAL 9000', 'AGENT SMITH'],\n",
    "    \"LIF\": [5245, 4609, 1797, 3396],\n",
    "    \"MVD\": [1, 1, 3, 4],\n",
    "    \"MDC\": [42, 30990, 6365, 1387],\n",
    "    \"MVT\": [4, 31186, 352020, 164069],\n",
    "    \"TLC\": [81, 31009, 351984, 154765],\n",
    "    \"NSD_1\": [3, 1, 666, 2699],\n",
    "    \"NSD_2\": [5, 2, 667, 2907],\n",
    "}\n",
    "\n",
    "# Create the Polars DataFrame\n",
    "champion_metrics_df = pl.DataFrame(champion_metrics)\n",
    "\n",
    "# Add the \"NSD\" column as the maximum of \"NSD_1\" and \"NSD_2\", then drop the original columns\n",
    "champion_metrics_df = champion_metrics_df.with_columns(\n",
    "    pl.max_horizontal([\"NSD_1\", \"NSD_2\"]).alias(\"NSD\")  # Compute max horizontally\n",
    ").drop([\"NSD_1\", \"NSD_2\"])  # Drop the original columns\n",
    "\n",
    "# Convert Polars DataFrame to Pandas DataFrame\n",
    "champion_metrics_df = champion_metrics_df.to_pandas()\n",
    "\n",
    "# Normalize only numeric columns and keep the 'group' column intact\n",
    "champion_metrics_df = champion_metrics_df.set_index(\"group\")  # Set 'group' as the index\n",
    "champion_metrics_df = champion_metrics_df / champion_metrics_df.max() * 99  # Normalize and scale\n",
    "champion_metrics_df = champion_metrics_df.round(0)  # Round values\n",
    "champion_metrics_df = champion_metrics_df.reset_index()  # Restore 'group' as a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- PART 1: Define a function that do a plot for one line of the dataset!\n",
    " \n",
    "def make_spider(row, title, color):\n",
    "\n",
    "    # number of variable\n",
    "    categories=list(champion_metrics_df)[1:]\n",
    "    N = len(categories)\n",
    "\n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(2,2,row+1, polar=True, )\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=8)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([10,20,30,40,50,60,70,80,90,100], [\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\",\"80\",\"90\",\"100\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "    # Ind1\n",
    "    values=champion_metrics_df.loc[row].drop('group').values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, color=color, alpha=0.4)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(title, size=11, color=color, y=1.1)\n",
    "\n",
    "    \n",
    "# ------- PART 2: Apply the function to all individuals\n",
    "# initialize the figure\n",
    "my_dpi=96\n",
    "plt.figure(figsize=(1000/my_dpi, 1000/my_dpi), dpi=my_dpi)\n",
    " \n",
    "# Create a color palette:\n",
    "my_palette = plt.cm.get_cmap(\"Set2\", len(df.index))\n",
    " \n",
    "# Loop to plot\n",
    "for row in range(0, len(champion_metrics_df.index)):\n",
    "    make_spider( row=row, title=\" \", color='red')\n",
    "plt.savefig('./image_aurel/radar_plts_champions.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- PART 1: Define a function that creates a radar plot -------\n",
    "def make_spider(row, title, color, output_path):\n",
    "    # Number of variables\n",
    "    categories = list(champion_metrics_df.columns[1:])  # Exclude 'group'\n",
    "    N = len(categories)\n",
    "\n",
    "    # Angles for the radar plot\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "\n",
    "    # Initialize the radar plot\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=96, subplot_kw={\"polar\": True})\n",
    "\n",
    "    # Adjust axis directions\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Add labels for axes\n",
    "    plt.xticks(angles[:-1], categories, color=\"grey\", size=8)\n",
    "\n",
    "    # Add y-labels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks(\n",
    "        [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "        [\"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\", \"100\"],\n",
    "        color=\"grey\",\n",
    "        size=7,\n",
    "    )\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # Data values\n",
    "    values = champion_metrics_df.iloc[row, 1:].values.flatten().tolist()\n",
    "    values += values[:1]  # Close the loop\n",
    "\n",
    "    # Plot the data\n",
    "    ax.plot(angles, values, color=color, linewidth=2, linestyle=\"solid\")\n",
    "    ax.fill(angles, values, color=color, alpha=0.4)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(title, size=11, color=color, y=1.1)\n",
    "\n",
    "    # Save the radar plot\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ------- PART 2: Combine radar plot with PNG image -------\n",
    "def combine_images(radar_path, png_path, output_path):\n",
    "    # Open both images\n",
    "    radar_img = Image.open(radar_path)\n",
    "    png_img = Image.open(png_path)\n",
    "\n",
    "    # Resize images to have the same height\n",
    "    png_img = png_img.resize((int(radar_img.width * 0.8), radar_img.height))\n",
    "\n",
    "    # Create a blank image for the composite\n",
    "    combined_width = radar_img.width + png_img.width\n",
    "    combined_img = Image.new(\"RGB\", (combined_width, radar_img.height))\n",
    "\n",
    "    # Paste the radar and PNG images side by side\n",
    "    combined_img.paste(png_img, (0, 0))  # Left side: PNG\n",
    "    combined_img.paste(radar_img, (png_img.width, 0))  # Right side: Radar plot\n",
    "\n",
    "    # Save the combined image\n",
    "    combined_img.save(output_path)\n",
    "\n",
    "\n",
    "# ------- PART 3: Main Loop -------\n",
    "# Folder containing PNG images\n",
    "image_folder = \"./fut\"  # Update with your folder path\n",
    "\n",
    "# Get a list of all PNG images in the folder\n",
    "png_files = sorted(\n",
    "    [os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith(\".png\")]\n",
    ")\n",
    "\n",
    "champion_metrics_df = pd.DataFrame(champion_metrics)\n",
    "\n",
    "# Create plots and combine images\n",
    "output_folder = \"./output\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "for row, png_path in zip(range(len(champion_metrics_df)), png_files):\n",
    "    radar_path = os.path.join(output_folder, f\"radar_{row + 1}.png\")\n",
    "    output_path = os.path.join(output_folder, f\"composite_{row + 1}.png\")\n",
    "\n",
    "    # Create radar plot\n",
    "    make_spider(row=row, title=f\"Group {champion_metrics_df['group'][row]}\", color=\"red\", output_path=radar_path)\n",
    "\n",
    "    # Combine radar plot with the corresponding PNG image\n",
    "    combine_images(radar_path, png_path, output_path)\n",
    "\n",
    "    print(f\"Created: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
