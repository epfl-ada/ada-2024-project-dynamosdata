{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <p style=\"text-align: center\"> Invisible Influencers </p>\n",
    "## <p style=\"text-align: center\"> Investigating YouTube Bot's Phenomenon </p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import glob\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ensure plotly.js is set to default\n",
    "pio.kaleido.scope.plotlyjs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Default Plotting Params\n",
    "\n",
    "font_family = \"Arial\"  # Replace with your desired font\n",
    "plt.rcParams[\"font.family\"] = font_family\n",
    "\n",
    "# Optional: Set font size and weight\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "\n",
    "# Apply Seaborn theme (inherits Matplotlib fonts)\n",
    "sns.set_theme(style=\"whitegrid\", font=font_family)\n",
    "\n",
    "# Apply Plotly theme\n",
    "plotly_layout = {\n",
    "    \"font\": {\n",
    "        \"family\": font_family,\n",
    "        \"size\": 12,  # Match with plt.rcParams[\"font.size\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center\"> <span style=\"text-decoration: underline\"> **Type-1 Bot Analysis** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Data Preprocessing and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "\n",
    "if combine:\n",
    "\n",
    "    datasets = ['normal', 'suspicious']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "        # List all Parquet files\n",
    "        parquet_files = glob.glob(f'./data_type1/{dataset}_users_*.parquet')\n",
    "\n",
    "        # Read and concatenate all Parquet files\n",
    "        combined = pl.concat([pl.read_parquet(file) for file in parquet_files])\n",
    "        combined.write_parquet(f'data_type1/combi_{dataset}_dataset1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "type_1=pl.read_parquet('data/data_type1/combi_dataset1.parquet')\n",
    "df_sus=pl.read_parquet('data/data_type1/combi_suspicious_dataset1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'year' to the dataframe\n",
    "\n",
    "type_1 = type_1.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])\n",
    "\n",
    "df_sus = df_sus.with_columns([\n",
    "    pl.col(\"upload_date\").cast(pl.Date),  # Ensure it's in datetime format\n",
    "    pl.col(\"upload_date\").dt.year().alias(\"year\")  # Extract year\n",
    "])\n",
    "\n",
    "df_sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Target Specific Video Categories ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colormap for consistency\n",
    "\n",
    "colormap_categories = {\n",
    "    \"Entertainment\": \"red\",\n",
    "    \"Other\": \"orange\",\n",
    "    \"Gaming\": \"cyan\",\n",
    "    \"People & Blogs\": \"yellow\",\n",
    "    \"News & Politics\": \"lime\",\n",
    "    \"Howto & Style\": \"lightblue\",\n",
    "    \"Music\": \"blue\",\n",
    "    \"Education\": \"lightgreen\",\n",
    "    \"Science & Technology\": \"brown\",\n",
    "    \"Film & Animation\": \"pink\",\n",
    "    \"Comedy\": \"green\",\n",
    "    \"Sports\": \"purple\",\n",
    "    \"Pets & Animals\": \"teal\",\n",
    "    \"Travel & Events\": \"lavender\",\n",
    "    \"Autos & Vehicles\": \"salmon\",\n",
    "    \"Nonprofits & Activism\": \"gold\",\n",
    "    \"Shows\": \"gold\",\n",
    "    \"Trailers\": \"lightcoral\"\n",
    "}\n",
    "\n",
    "order_categories = {\n",
    "    \"Entertainment\": 1,\n",
    "    \"Gaming\": 2,\n",
    "    \"People & Blogs\": 3,\n",
    "    \"News & Politics\": 4,\n",
    "    \"Howto & Style\": 5,\n",
    "    \"Music\": 6,\n",
    "    \"Education\": 7,\n",
    "    \"Science & Technology\": 8,\n",
    "    \"Film & Animation\": 9,\n",
    "    \"Comedy\": 10,\n",
    "    \"Sports\": 11,\n",
    "    \"Pets & Animals\": 12,\n",
    "    \"Travel & Events\": 13,\n",
    "    \"Autos & Vehicles\": 14,\n",
    "    \"Nonprofits & Activism\": 15,\n",
    "    \"Shows\": 16,\n",
    "    \"Trailers\": 17,\n",
    "    \"Other\": 18\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of comments per category of videos\n",
    "df_comm_per_cat = df_sus.group_by([\"year\",\"categories\"]).agg(pl.col(\"comments\").sum()).filter(pl.col(\"categories\")!=\"\").sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add proportion of comments per category per year\n",
    "comments_per_year = df_comm_per_cat.group_by(\"year\").agg(pl.col(\"comments\").sum().alias(\"total_comments\")).sort(by=\"year\")\n",
    "df_comm_per_cat = df_comm_per_cat.join(comments_per_year, on=\"year\")\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns([\n",
    "    (pl.col(\"comments\") / pl.col(\"total_comments\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "# Keep only categories with more than 2% of the comments, put others in a category 'Other'\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns(pl.when(pl.col(\"proportion\")<5.0).then(pl.lit(\"Other\")).otherwise(pl.col(\"categories\")).alias(\"Categories\")).drop(\"categories\")\n",
    "df_comm_per_cat = df_comm_per_cat.group_by([\"year\",\"Categories\"]).agg(pl.col(\"comments\").sum(), pl.col(\"proportion\").sum()).sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add order column for plotting\n",
    "df_comm_per_cat = df_comm_per_cat.with_columns([\n",
    "    pl.col(\"Categories\").map_elements(lambda x: order_categories[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_comm_per_cat = df_comm_per_cat.sort(by=[\"year\",\"order\"])\n",
    "\n",
    "df_comm_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_comm_per_cat.to_pandas(), x=\"year\", y=\"proportion\", color=\"Categories\", \n",
    "              labels={\"proportion\":\"Proportion of Comments (%)\", \"year\":\"Year\", \"Categories\":\"Category\"},\n",
    "              color_discrete_map=colormap_categories)\n",
    "fig.show()\n",
    "fig.write_image(\"./image_aurel/prop_comments_per_category_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_comments_per_category_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of comments per category of videos\n",
    "df_comm_per_cat_normal = type_1.group_by([\"year\",\"categories\"]).agg(pl.col(\"comments\").sum()).filter(pl.col(\"categories\")!=\"\").sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add proportion of comments per category per year\n",
    "comments_per_year_normal = df_comm_per_cat_normal.group_by(\"year\").agg(pl.col(\"comments\").sum().alias(\"total_comments\")).sort(by=\"year\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.join(comments_per_year_normal, on=\"year\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns([\n",
    "    (pl.col(\"comments\") / pl.col(\"total_comments\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "# Keep only categories with more than 4% of the comments, put others in a category 'Other'\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns(pl.when(pl.col(\"proportion\")<5.0).then(pl.lit(\"Other\")).otherwise(pl.col(\"categories\")).alias(\"Categories\")).drop(\"categories\")\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.group_by([\"year\",\"Categories\"]).agg(pl.col(\"comments\").sum(), pl.col(\"proportion\").sum()).sort(by=[\"year\",\"comments\"], descending=True)\n",
    "\n",
    "# Add order column for plotting\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.with_columns([\n",
    "    pl.col(\"Categories\").map_elements(lambda x: order_categories[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_comm_per_cat_normal = df_comm_per_cat_normal.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_comm_per_cat_normal.to_pandas(), x=\"year\", y=\"proportion\", color=\"Categories\", \n",
    "            labels={\"proportion\":\"Proportion of Comments (%)\", \"year\":\"Year\", \"Categories\":\"Category\"},\n",
    "            color_discrete_map=colormap_categories)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_comments_per_category_normal.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_comments_per_category_normal.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Target One or Many Channels ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map for consistency\n",
    "\n",
    "colormap_cat_channel = {\n",
    "    \"1\": \"red\",\n",
    "    \"2\": \"blue\",\n",
    "    \"3\": \"green\",\n",
    "    \"4\": \"orange\",\n",
    "    \"5+\": \"cyan\",\n",
    "}\n",
    "\n",
    "order_cat_channel = {\n",
    "    \"1\": 1,\n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5+\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_channels_per_bot = df_sus.group_by([\"year\",\"author\"]).agg(pl.col(\"channel_id\").n_unique().alias(\"nb_channels\")).sort(by=[\"year\",\"nb_channels\"], descending=True)\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns(pl.when(pl.col(\"nb_channels\")>=5).then(pl.lit(\"5+\")).otherwise(pl.col(\"nb_channels\")).alias(\"nb_channels\")).group_by([\"year\",\"nb_channels\"]).agg(pl.col(\"nb_channels\").count().alias(\"nb_users\")).sort(by=[\"year\",\"nb_users\"], descending=True)\n",
    "\n",
    "# Proportion per year \n",
    "\n",
    "nb_bots_per_year = df_nb_channels_per_bot.group_by(\"year\").agg(pl.col(\"nb_users\").sum().alias(\"total_users\")).sort(by=\"year\")\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.join(nb_bots_per_year, on=\"year\")\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns([\n",
    "    (pl.col(\"nb_users\") / pl.col(\"total_users\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.with_columns([\n",
    "    pl.col(\"nb_channels\").map_elements(lambda x: order_cat_channel[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_bot = df_nb_channels_per_bot.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_nb_channels_per_bot.to_pandas(), x=\"year\", y=\"proportion\", color=\"nb_channels\", \n",
    "              labels={\"proportion\":\"Proportion of Bots (%)\", \"year\":\"Year\", \"nb_channels\":\"Channels Targeted\"},\n",
    "              color_discrete_map=colormap_cat_channel)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_bots_per_channels_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_bots_per_channels_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_channels_per_user = type_1.group_by([\"year\",\"author\"]).agg(pl.col(\"channel_id\").n_unique().alias(\"nb_channels\")).sort(by=[\"year\",\"nb_channels\"], descending=True)\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns(pl.when(pl.col(\"nb_channels\")>=5).then(pl.lit(\"5+\")).otherwise(pl.col(\"nb_channels\")).alias(\"nb_channels\")).group_by([\"year\",\"nb_channels\"]).agg(pl.col(\"nb_channels\").count().alias(\"nb_users\")).sort(by=[\"year\",\"nb_users\"], descending=True)\n",
    "\n",
    "# Proportion per year \n",
    "\n",
    "nb_user_per_year = df_nb_channels_per_user.group_by(\"year\").agg(pl.col(\"nb_users\").sum().alias(\"total_users\")).sort(by=\"year\")\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.join(nb_user_per_year, on=\"year\")\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns([\n",
    "    (pl.col(\"nb_users\") / pl.col(\"total_users\") * 100.0).alias(\"proportion\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.with_columns([\n",
    "    pl.col(\"nb_channels\").map_elements(lambda x: order_cat_channel[x]).alias(\"order\")\n",
    "])\n",
    "\n",
    "df_nb_channels_per_user = df_nb_channels_per_user.sort(by=[\"year\",\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(df_nb_channels_per_user.to_pandas(), x=\"year\", y=\"proportion\", color=\"nb_channels\", \n",
    "              labels={\"proportion\":\"Proportion of Normal users (%)\", \"year\":\"Year\", \"nb_channels\":\"Channels Targeted\"},\n",
    "              color_discrete_map=colormap_cat_channel)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig.write_image(\"./image_aurel/prop_normal_per_channels.svg\")\n",
    "fig.write_html(\"./image_aurel/prop_normal_per_channels.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Different are Normal Users and Bots in Commenting Behaviors ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type1_comm_per_user = type_1.select([\"author\", \"comments\"]).group_by(\"author\").agg(pl.col(\"comments\").sum()).sort(by=\"comments\", descending=True)\n",
    "\n",
    "df_sus_comm_per_bot= df_sus.select([\"author\", \"comments\"]).group_by(\"author\").agg(pl.col(\"comments\").sum()).sort(by=\"comments\", descending=True)\n",
    "\n",
    "df_comm_per_bot = pl.concat([data_type1_comm_per_user.with_columns([\n",
    "    pl.lit(\"Normal\").alias(\"Type\")\n",
    "]), df_sus_comm_per_bot.with_columns([\n",
    "    pl.lit(\"Suspicious\").alias(\"Type\")\n",
    "])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot of comments per user for normal and suspicious users\n",
    "\n",
    "fig = px.box(df_comm_per_bot.to_pandas(), x=\"Type\", y=\"comments\", color=\"Type\",\n",
    "         labels={\"comments\":\"Number of Comments\", \"Type\":\"User Type\"},\n",
    "         log_y=True)\n",
    "fig.update_yaxes(range=[None, 10**5])\n",
    "\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"./image_aurel/boxplot_comments_per_user.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How Do Metrics Vary Over Time For Normal Users and Bots ?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Metrics Data\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size_sus = 1_000_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results_sus = []\n",
    "chunk_results_sus = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start_sus in range(0, len(df_sus), chunk_size_sus):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus[start_sus : start_sus + chunk_size_sus]\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill_sus = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col_sus in columns_to_fill_sus:\n",
    "        if col_sus in chunk_sus.columns:\n",
    "            chunk_sus = chunk_sus.with_columns(pl.col(col_sus).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results_sus.append(chunk_metrics_sus)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics_sus = pl.concat(chunk_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start_sus in range(0, len(df_sus), chunk_size_sus):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk_sus = df_sus[start_sus : start_sus + chunk_size_sus]\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users_sus = (\n",
    "        chunk_sus.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results_sus.append(unique_users_sus)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined_sus = pl.concat(unique_users_results_sus).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result_sus = final_metrics_sus.join(unique_users_combined_sus, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result_sus = final_result_sus.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")\n",
    "# Sort the DataFrame by year\n",
    "final_result_sus = final_result_sus.sort(\"year\")\n",
    "# Display the final result\n",
    "print(final_result_sus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics\n",
    "\n",
    "final_result_sus = final_result_sus.with_columns([\n",
    "    (pl.col(\"total_likes\") / final_result_sus[\"total_likes\"].max()).alias(\"Total Likes\"),\n",
    "    (pl.col(\"total_comments\") / final_result_sus[\"total_comments\"].max()).alias(\"Total Comments\"),\n",
    "    (pl.col(\"comments_per_user\") / final_result_sus[\"comments_per_user\"].max()).alias(\"Comments per User\"),\n",
    "    (pl.col(\"total_replies\") / final_result_sus[\"total_replies\"].max()).alias(\"Total Replies\"),\n",
    "])\n",
    "\n",
    "# Remove year 2005\n",
    "final_result_sus = final_result_sus.filter(pl.col(\"year\")>2005)\n",
    "\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"cyan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(final_result_sus.to_pandas(), x=\"year\", y=[\"Total Likes\", \"Total Comments\", \"Comments per User\", \"Total Replies\"],\n",
    "                labels={\"value\":\"Normalized Value\", \"year\":\"Year\", \"variable\":\"Metric\"},\n",
    "                color_discrete_sequence=colors, markers=True)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = [2006+x for x in range(0,13, 2)],\n",
    "        ticktext = [str(2006+x) for x in range(0,13, 2)]\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"./image_aurel/normalized_metrics_sus.svg\")\n",
    "fig.write_html(\"./image_aurel/normalized_metrics_sus.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Metrics Data\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1_000_000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize lists to collect results\n",
    "unique_users_results = []\n",
    "chunk_results = []\n",
    "\n",
    "# Iterate through the dataset in chunks\n",
    "for start in range(0, len(type_1), chunk_size):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = type_1[start : start + chunk_size]\n",
    "\n",
    "    # Fill missing values in specific columns (only if they exist)\n",
    "    columns_to_fill = [\"comments\", \"likes\", \"replies\", \"view_count\"]\n",
    "    for col in columns_to_fill:\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk.with_columns(pl.col(col).fill_null(0))\n",
    "\n",
    "    # Aggregate metrics for this chunk\n",
    "    chunk_metrics = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg([\n",
    "            pl.col(\"comments\").sum().alias(\"total_comments\"),\n",
    "            pl.col(\"likes\").sum().alias(\"total_likes\"),\n",
    "            pl.col(\"replies\").sum().alias(\"total_replies\"),\n",
    "            pl.col(\"view_count\").sum().alias(\"total_views\"),\n",
    "            pl.col(\"video_id\").count().alias(\"total_videos\"),\n",
    "        ])\n",
    "    )\n",
    "    chunk_results.append(chunk_metrics)\n",
    "\n",
    "# Combine all chunk results for aggregated metrics\n",
    "final_metrics = pl.concat(chunk_results).group_by(\"year\").sum()\n",
    "\n",
    "# Iterate through the dataset again for unique users\n",
    "for start in range(0, len(type_1), chunk_size):\n",
    "    # Slice the DataFrame for the current chunk\n",
    "    chunk = type_1[start : start + chunk_size]\n",
    "\n",
    "    # Calculate unique users per year\n",
    "    unique_users = (\n",
    "        chunk.group_by(\"year\")\n",
    "        .agg(pl.col(\"author\").n_unique().alias(\"unique_users\"))\n",
    "    )\n",
    "    unique_users_results.append(unique_users)\n",
    "\n",
    "# Combine all unique users results\n",
    "unique_users_combined = pl.concat(unique_users_results).group_by(\"year\").sum()\n",
    "\n",
    "# Merge the aggregated metrics with unique users\n",
    "final_result = final_metrics.join(unique_users_combined, on=\"year\", how=\"left\")\n",
    "\n",
    "# Compute comments per user\n",
    "final_result = final_result.with_columns(\n",
    "    (pl.col(\"total_comments\") / pl.col(\"unique_users\")).alias(\"comments_per_user\")\n",
    ")\n",
    "\n",
    "# Display the final result\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Metrics\n",
    "\n",
    "final_result = final_result.with_columns([\n",
    "    (pl.col(\"total_likes\") / final_result[\"total_likes\"].max()).alias(\"Total Likes\"),\n",
    "    (pl.col(\"total_comments\") / final_result[\"total_comments\"].max()).alias(\"Total Comments\"),\n",
    "    (pl.col(\"comments_per_user\") / final_result[\"comments_per_user\"].max()).alias(\"Comments per User\"),\n",
    "    (pl.col(\"total_replies\") / final_result[\"total_replies\"].max()).alias(\"Total Replies\"),\n",
    "])\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"cyan\"]\n",
    "markers = {\n",
    "    \"Total Likes\": \"circle\",\n",
    "    \"Total Comments\": \"square\",\n",
    "    \"Comments per User\": \"diamond\",\n",
    "    \"Total Replies\": \"triangle-up\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(final_result.to_pandas(), x=\"year\", y=[\"Total Likes\", \"Total Comments\", \"Comments per User\", \"Total Replies\"],\n",
    "                labels={\"value\":\"Normalized Value\", \"year\":\"Year\", \"variable\":\"Metric\"},\n",
    "                color_discrete_sequence=colors, markers=True)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = [2006+x for x in range(0,13, 2)],\n",
    "        ticktext = [str(2006+x) for x in range(0,13, 2)]\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"./image_aurel/normalized_metrics.svg\")\n",
    "fig.write_html(\"./image_aurel/normalized_metrics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **How different are bots and normal users?** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Suspicious Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter suspicious users to retain only the necessary columns\n",
    "data_com_vid_year_sus= df_sus.select([\"year\", \"author\", \"comments\", \"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and author to compute metrics\n",
    "\n",
    "grouped_data_sus = (\n",
    "    data_com_vid_year_sus.group_by([\"year\", \"author\"])  # Group by year and author\n",
    "    .agg([\n",
    "        pl.col(\"comments\").sum().alias(\"total_comments\"),  # Total comments by user per year\n",
    "        pl.col(\"video_id\").n_unique().alias(\"distinct_videos_commented\"),  # Unique videos commented on\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"total_comments\") / pl.col(\"distinct_videos_commented\")).alias(\"avg_comments_per_user\")  # Avg comments per user\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample suspicious users because the plot would be impossible to make :\n",
    "\n",
    "# Group data by year\n",
    "grouped_sus = grouped_data_sus.group_by('year')\n",
    "\n",
    "# Calculate the total subsample size (1/100th of the total rows)\n",
    "total_subsample_size = len(grouped_data_sus) // 1\n",
    "\n",
    "# Subsampling uniformly across years\n",
    "subsampled_list_sus = []\n",
    "for year, group in grouped_sus:\n",
    "    # Ensure `group` is a DataFrame\n",
    "    group = pd.DataFrame(group)\n",
    "    \n",
    "    # Calculate the number of samples for this year (proportional to the group's size)\n",
    "    year_sample_size = max(1, len(group) * total_subsample_size // len(grouped_data_sus))\n",
    "    \n",
    "    # Randomly sample the data for this year\n",
    "    subsampled_list_sus.append(group.sample(n=year_sample_size, random_state=42))\n",
    "\n",
    "# Combine the sampled data from all years\n",
    "subsampled_df_sus = pd.concat(subsampled_list_sus)\n",
    "\n",
    "# Define the correct column names\n",
    "column_names = ['year', 'author', 'total_comments', 'distinct_videos_commented', 'avg_comments_per_user']\n",
    "\n",
    "# Assign the column names back to the DataFrame\n",
    "subsampled_df_sus.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the year column is integer for proper sorting FOR SUSPICIOUS USERS\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "subsampled_df_sus = subsampled_df_sus.sort_values(by='year', ascending=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "subsampled_df_sus = subsampled_df_sus.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Line Plot for Suspicious Users\n",
    "\n",
    "# Ensure Seaborn has a clean theme\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Ensure 'year' is treated as categorical for the plot\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(int)  # Convert years to integers\n",
    "subsampled_df_sus['year'] = subsampled_df_sus['year'].astype(str)  # Convert to string for categorical use\n",
    "\n",
    "# Generate a color palette for years\n",
    "pal = sns.color_palette(palette='viridis', n_colors=subsampled_df_sus['year'].nunique())\n",
    "\n",
    "# Create the FacetGrid for ridgeline\n",
    "g = sns.FacetGrid(\n",
    "    subsampled_df_sus,\n",
    "    row='year',\n",
    "    hue='year',\n",
    "    aspect=15,  # Stretch plots horizontally\n",
    "    height=0.5,  # Adjust height of each row\n",
    "    palette=pal,\n",
    ")\n",
    "\n",
    "# Add density plots (kde)\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=0.2,\n",
    "    clip=(0, 50),  # Clip x-axis range to 0-5\n",
    "    #clip_on=False,\n",
    "    fill=True,\n",
    "    alpha=1,\n",
    "    linewidth=1.5,\n",
    ")\n",
    "# g.set_titles(\"\")\n",
    "# g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Add a white contour line around each density plot\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=0.2,\n",
    "    clip=(0, 50),  # Clip x-axis range to 0-5\n",
    "    #clip_on=False,  \n",
    "    color=\"w\",\n",
    "    lw=2,\n",
    ")\n",
    "\n",
    "# Add a horizontal line at y=0 for each plot\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Add year labels to each plot\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.text(\n",
    "        -0.5, 0.02,  # Adjust the position of the year label\n",
    "        subsampled_df_sus['year'].unique()[i],\n",
    "        fontweight='bold',\n",
    "        fontsize=12,\n",
    "        color=ax.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "# Adjust subplot overlap\n",
    "g.fig.subplots_adjust(hspace=-0.5)\n",
    "\n",
    "# Remove the density label from each y-axis\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylabel(\"\")  # Remove the y-axis labels for density\n",
    "    ax.set_xlim(0, 50)  # Set x-axis range to 0 - 50    \n",
    "\n",
    "\n",
    "# Remove unnecessary axes details\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel(\"Average Comments per User\", fontweight='bold', fontsize=12)\n",
    "g.fig.suptitle('Distribution of Avg Comments per User Across Years',\n",
    "               ha='right', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "g.savefig(\"./image_aurel/ridge_line_sus.svg\")\n",
    "g.savefig(\"./image_aurel/ridge_line_sus.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Normal Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter normal users to retain only the necessary columns\n",
    "data_com_vid_year= type_1.select([\"year\", \"author\", \"comments\", \"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and author to compute metrics\n",
    "grouped_data = (\n",
    "    data_com_vid_year.group_by([\"year\", \"author\"])  # Group by year and author\n",
    "    .agg([\n",
    "        pl.col(\"comments\").sum().alias(\"total_comments\"),  # Total comments by user per year\n",
    "        pl.col(\"video_id\").n_unique().alias(\"distinct_videos_commented\"),  # Unique videos commented on\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"total_comments\") / pl.col(\"distinct_videos_commented\")).alias(\"avg_comments_per_user\")  # Avg comments per user\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample normal users because the plot would be impossible to make :\n",
    "\n",
    "# Group data by year\n",
    "grouped = grouped_data.group_by('year')\n",
    "\n",
    "# Calculate the total subsample size (1/100th of the total rows)\n",
    "total_subsample_size = len(grouped_data) // 100\n",
    "\n",
    "# Subsampling uniformly across years\n",
    "subsampled_list = []\n",
    "for year, group in grouped:\n",
    "    # Ensure `group` is a DataFrame\n",
    "    group = pd.DataFrame(group)\n",
    "    \n",
    "    # Calculate the number of samples for this year (proportional to the group's size)\n",
    "    year_sample_size = max(1, len(group) * total_subsample_size // len(grouped_data))\n",
    "    \n",
    "    # Randomly sample the data for this year\n",
    "    subsampled_list.append(group.sample(n=year_sample_size, random_state=42))\n",
    "\n",
    "# Combine the sampled data from all years\n",
    "subsampled_df = pd.concat(subsampled_list)\n",
    "\n",
    "# Define the correct column names\n",
    "column_names = ['year', 'author', 'total_comments', 'distinct_videos_commented', 'avg_comments_per_user']\n",
    "\n",
    "# Assign the column names back to the DataFrame\n",
    "subsampled_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Line Plot for Normal Users\n",
    "\n",
    "# Ensure the year column is integer for proper sorting\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "subsampled_df = subsampled_df.sort_values(by='year', ascending=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "subsampled_df = subsampled_df.reset_index(drop=True)\n",
    "\n",
    "# Ensure Seaborn has a clean theme\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Prepare data (already in `subsampled_df`)\n",
    "# Ensure 'year' is treated as categorical for the plot\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(int)  # Convert years to integers\n",
    "subsampled_df['year'] = subsampled_df['year'].astype(str)  # Convert to string for categorical use\n",
    "\n",
    "# Generate a color palette for years\n",
    "pal = sns.color_palette(palette='viridis', n_colors=subsampled_df['year'].nunique())\n",
    "\n",
    "# Create the FacetGrid for ridgeline\n",
    "g = sns.FacetGrid(\n",
    "    subsampled_df,\n",
    "    row='year',\n",
    "    hue='year',\n",
    "    aspect=15,  # Stretch plots horizontally\n",
    "    height=0.5,  # Adjust height of each row\n",
    "    palette=pal,\n",
    ")\n",
    "\n",
    "# Add density plots (kde)\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=1,  # Bandwidth adjustment\n",
    "    clip_on=False,\n",
    "    fill=True,\n",
    "    alpha=1,\n",
    "    linewidth=1.5,\n",
    ")\n",
    "# g.set_titles(\"\")\n",
    "# g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Add a white contour line around each density plot\n",
    "g.map(\n",
    "    sns.kdeplot,\n",
    "    'avg_comments_per_user',\n",
    "    bw_adjust=1,\n",
    "    clip_on=False,  \n",
    "    color=\"w\",\n",
    "    lw=2,\n",
    ")\n",
    "\n",
    "# Add a horizontal line at y=0 for each plot\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Add year labels to each plot\n",
    "for i, ax in enumerate(g.axes.flat):\n",
    "    ax.text(\n",
    "        -0.5, 0.02,  # Adjust the position of the year label\n",
    "        subsampled_df['year'].unique()[i],\n",
    "        fontweight='bold',\n",
    "        fontsize=12,\n",
    "        color=ax.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "# Adjust subplot overlap\n",
    "g.fig.subplots_adjust(hspace=-0.5)\n",
    "\n",
    "# Remove the density label from each y-axis\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylabel(\"\")  # Remove the y-axis labels for density\n",
    "\n",
    "# Remove unnecessary axes details\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel(\"Average Comments per User\", fontweight='bold', fontsize=12)\n",
    "g.fig.suptitle('Distribution of Avg Comments per User Across Years',\n",
    "               ha='right', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "g.savefig(\"./image_aurel/ridge_line_normal.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **3D Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine both datasets for SVM training\n",
    "combined_df = pd.concat([\n",
    "    subsampled_df.assign(label=0),  # Normal users\n",
    "    subsampled_df_sus.assign(label=1)  # Suspicious users\n",
    "])\n",
    "\n",
    "# Combine features and labels\n",
    "X = combined_df[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values\n",
    "y = combined_df[\"label\"].values\n",
    "\n",
    "# Train the SVM\n",
    "svm = SVC(kernel=\"linear\", C=1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Extract hyperplane parameters\n",
    "w = svm.coef_[0]  # Weights\n",
    "b = svm.intercept_[0]  # Intercept\n",
    "\n",
    "# Create mesh grid for the hyperplane\n",
    "x_range = np.linspace(combined_df[\"year\"].min(), combined_df[\"year\"].max(), 30)\n",
    "y_range = np.linspace(combined_df[\"avg_comments_per_user\"].min(), combined_df[\"avg_comments_per_user\"].max(), 30)\n",
    "x, y = np.meshgrid(x_range, y_range)\n",
    "z = (-w[0] * x - w[1] * y - b) / w[2]  # Solve for z\n",
    "\n",
    "# Density calculation for Dataset 1\n",
    "xyz1 = subsampled_df[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values.T\n",
    "kde1 = gaussian_kde(xyz1)(xyz1)\n",
    "subsampled_df[\"density\"] = kde1\n",
    "\n",
    "# Density calculation for Dataset 2\n",
    "xyz2 = subsampled_df_sus[[\"year\", \"avg_comments_per_user\", \"distinct_videos_commented\"]].values.T\n",
    "kde2 = gaussian_kde(xyz2)(xyz2)\n",
    "subsampled_df_sus[\"density\"] = kde2\n",
    "\n",
    "# Create the 3D Scatter Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Dataset 1 (Normal Users) with density-based coloring\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=subsampled_df[\"year\"],\n",
    "    y=subsampled_df[\"avg_comments_per_user\"],\n",
    "    z=subsampled_df[\"distinct_videos_commented\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=subsampled_df[\"density\"],  # Color based on density\n",
    "        colorscale=\"Viridis\",  # Blue-Green color scale\n",
    "        opacity=0.6\n",
    "    ),\n",
    "    name=\"Normal Users\"\n",
    "))\n",
    "\n",
    "# Add Dataset 2 (Suspicious Users) with density-based coloring\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=subsampled_df_sus[\"year\"],\n",
    "    y=subsampled_df_sus[\"avg_comments_per_user\"],\n",
    "    z=subsampled_df_sus[\"distinct_videos_commented\"],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=subsampled_df_sus[\"density\"],  # Color based on density\n",
    "        colorscale=\"Jet\",  # Yellow-to-Red color scale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name=\"Suspicious Users\"\n",
    "))\n",
    "\n",
    "# Add the SVM hyperplane\n",
    "fig.add_trace(go.Surface(\n",
    "    x=x, y=y, z=z,\n",
    "    colorscale=[[0, 'lightgrey'], [1, 'lightgrey']],  # Single light-grey color\n",
    "    opacity=0.5,\n",
    "    showscale=False,  # Remove colorbar for hyperplane\n",
    "    name=\"SVM Hyperplane\"\n",
    "))\n",
    "\n",
    "# Add a box with the hyperplane metrics\n",
    "fig.add_annotation(\n",
    "    text=f\"<b>SVM Hyperplane Metrics</b><br>\"\n",
    "         f\"Equation: {w[0]:.2f}*x1 + {w[1]:.2f}*x2 + {w[2]:.2f}*x3 + {b:.2f} = 0\",\n",
    "    showarrow=False,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.05, y=0.95,  # Position: top-left corner\n",
    "    bordercolor=\"black\", borderwidth=1,\n",
    "    bgcolor=\"white\", font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"Year\", tickvals=list(range(2005, 2020)), autorange=\"reversed\"),\n",
    "        yaxis=dict(title=\"Average Comments/User\", range=[0, 50]),\n",
    "        zaxis=dict(title=\"Distinct Videos Commented\", range=[0, 1000])\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "fig.write_html(\"./image_aurel/3D_hyperplane_density.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center\"> <span style=\"text-decoration: underline\"> **Type-2 Bot Analysis** </span> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Data Preprocessing and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = False\n",
    "\n",
    "if combine:\n",
    "\n",
    "    datasets = ['normal', 'suspicious']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"reading partial files '{dataset}_i.parquet'...\")\n",
    "\n",
    "        # List all Parquet files\n",
    "        parquet_files = glob.glob(f'./data/data_type2/{dataset}_users_*.parquet')\n",
    "\n",
    "        # Read and concatenate all Parquet files\n",
    "        combined = pl.concat([pl.read_parquet(file) for file in parquet_files])\n",
    "        combined.write_parquet(f'./data/data_type2/combi_{dataset}_dataset2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_2 = pl.read_parquet('./data/data_type2/combi_normal_dataset2.parquet')\n",
    "df_sus_2 = pl.read_parquet('./data/data_type2/combi_suspicious_dataset2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **Do Bots Comment On More Videos ?** </span> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.read_parquet(df_normal_2)[0] # Just need 1 df for plot\n",
    "df_sus =  pd.read_parquet(df_sus_2)[0]\n",
    "df_sus = df_sus[df_sus[\"videos_commented\"] >= 10]\n",
    "\n",
    "# Adding a column to discriminate between the 2 df\n",
    "df_norm[\"category\"]='normal'\n",
    "df_sus[\"category\"]='suspicious'\n",
    "\n",
    "# Concatenating for the plot\n",
    "df = pd.concat([df_norm[::5],df_sus[df_sus[\"videos_commented\"] < 30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.violinplot(x=\"category\", y=\"videos_commented\", hue=\"category\", data=df, palette=\"Pastel1\", split=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align: center; margin-top: 0.5cm\"> <span style=\"text-decoration: underline\"> **BOT RANKING** </span> </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lifetime of suspicious users\n",
    "\n",
    "lifetime_df = df_sus.group_by(\"author\").agg([\n",
    "                pl.col(\"upload_date\").min().alias(\"min\"),\n",
    "                pl.col(\"upload_date\").max().alias(\"max\")])\n",
    "\n",
    "# Compute the difference\n",
    "date_diff = (lifetime_df['max'] - lifetime_df['min']).alias(\"date_diff\")\n",
    "\n",
    "# Create the constant of one day to add\n",
    "one_day = pl.duration(days=1).alias(\"one_day\")\n",
    "\n",
    "lifetime_df = lifetime_df.with_columns([\n",
    "    (date_diff + one_day).alias(\"lifetime\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longuest_survival_type1=lifetime_df[\"lifetime\"].max()\n",
    "longest_survival_row_type1 = lifetime_df.filter(pl.col(\"lifetime\") == pl.col(\"lifetime\").max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
